[ 最近 一次 更新   17 - 11 - 28   10 : 30   ( UTC + 8 ) 。 ,   补 一张 很 代表性 的 图 ， 来自 11 月 28 日 纽约时报   https : / / www . nytimes . com / 2017 / 11 / 28 / technology / artificial - intelligence - research - toronto . html ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 48f538046fc5bc28075d5158d3b836db _ b . jpg "   data - size = " normal "   data - rawwidth = " 2048 "   data - rawheight = " 1365 "   class = " origin _ image   zh - lightbox - thumb "   width = " 2048 "   data - original = " https : / / pic4 . zhimg . com / v2 - 48f538046fc5bc28075d5158d3b836db _ r . jpg " / > Hinton   和 同事 拿 着   “ 四面体 ” ， 这个 四面体 给   Hinton   很多 关于 Capsules 的 启示 ， 详见 后文 ,   关于 最新 的   Hinton   的 论文   Dynamic   Routing   Between   Capsules ， 参见   https : / / www . zhihu . com / question / 67287444 / answer / 251241736 。 这篇 论文 是 对 Capsule 最 初步 的 实现 ， 个人 认为 写 得 很 随意 。 ,   " 上 一次 更新   17 - 10 - 11   11 : 00   ( UTC + 8 ) 。 改善 了 一些 表述 ， 在 无 监督 学习 部分 加入 了 Tufas   相关 内容 ， 以及 视觉 皮层 的 结构 。 " ,   上 一次 更新   17 - 09 - 22   15 : 00   （ 按 中国 时间 计 ） 。 修复 了 一些 笔误 ， 加入 了 更 多 关于 无 监督 学习 的 介绍 内容 ， 使 思路 更 完整 ； 以及 一两句 关于   Capsule   实际效果 的 消息 。 ,   这有 可能 也 是 知乎 上面 分析 介绍 深度 学习 最为 全面 的 文章 之一 。 希望 做 物理 的 ， 做 数学 的 ， 做 生物 的 ， 做 化学 的 ， 做 计算机 ， 包括 做 科幻 的 都 能 看 的 很 开心 。 ,   Hinton   以 “ 深度 学习 之 父 ”   和   “ 神经网络 先驱 ”   闻名于世 ， 其 对 深度 学习 及 神经网络 的 诸多 核心 算法 和 结构 （ 包括 “ 深度 学习 ” 这个 名称 本身 ， 反向 传播 算法 ， 受限 玻尔兹曼 机 ， 深度 置信 网络 ， 对比 散度 算法 ， ReLU 激活 单元 ， Dropout 防止 过 拟合 ， 以及 深度 学习 早期 在 语音 方面 突破 ） 做出 了 基础性 的 贡献 。 尽管 已经 将 大半辈子 的 时间 投入 到 神经网络 之上 ， 这位 老人 却 丝毫 没有 想 退休 的 意思 。 ,   Hinton   近几年 以   “ 卷积 神经网络 有 什么 问题 ？ ”   为 主题 做 了 多场 报道   [ 1 ]   [ 2 ] ， 提出 了 他 的   Capsule   计划 。 Hinton 似乎 毫不掩饰 要 推翻 自己 盼 了 30 多年 时间 才 建立 起来 的 深度 学习 帝国 的 想法   [ 3 ] 。 他 的 这种 精神 也 获得 了 同 行李 飞飞 （ ImageNet 创始者 ） 等 人 肯定   [ 4 ] 。 ,   Hinton   为什么 突然 想要 推倒重来 ？ 这 肯定 不是 出于 巧合 或者 突然 心血来潮 ， 毕竟 作为 一个 领域 的 先驱 ， 质疑 自己 亲手 建立 的 理论 ， 不是 谁 都 愿意 做 的 事情 。 （ 试想 一下 ， 如果 你 到处 做 报告 ， 说 自己 的 领域 有 各种各样 的 问题 ， 就算 不会 影响 到 自己 ， 也 让 做 这个 领域 的 同行 和 靠 这个 领域 吃饭 的 人 不是 很 舒服 ） ,   说 推倒重来 有点 过分 ， Hinton 并 没有 否定一切 ， 并且 他 的 主要 攻击 目标 是 深度 学习 在 计算机 视觉 方面 的 理论 。 但是 从 几次 演讲 来看 ， 他 的   Capsule   计划 确实 和 以前 的 方法 出入 比较 大 。 Hinton   演讲 比较 风趣 ， 但是 也 存在 思维 跳跃 ， 难度 跨度 太大 等 问题 。 这些 问题 在 他 的 关于   Capsule   的 报告 中 还是 比较突出 的 。 可以 说 仅仅 看 报告 很难 理解 完全   Hinton   的 想法 。 我 这 几天 结合 各类 资料 ， 整理 了 一下   Hinton   的 思路 和 动机 ， 和 大家 分享 一下 。 ,   ,   Hinton   与 神经网络 ,   （ 以下 用 NN 指代 人工神经网络 ， CNN 指代 （ 深度 ） 卷积 神经网络 ， DNN 指代 深度 神经网络 ） ,   要 深入 理解 Hinton 的 想法 ， 就 必须 了解 神经网络 发展 的 历史 ， 这 也 几乎 是 Hinton 的 学术史 。 ,   人工智能 才 起步 的 时候 ， 科学家 们 很 自然 的 会 有 模拟 人脑 的 想法 （ 被 称为 连接 主义 ） ， 因为 人脑 是 我们 唯一 知道 的 拥有 高级 智能 的 实体 。 ,   " NN   起源于 对 神经系统 的 模拟 ， 最早 的 形式 是 感知机 ， 学习 方法 是 神经 学习 理论 中 著名 的   Hebbs   rule   。 NN 最初 提出 就 成为 了 人工智能 火热 的 研究 方向 。 不过   Hebbs   rule   只能 训练 单层 NN ， 而 单层 NN 甚至 连 简单 的 “ 异或 ” 逻辑 都 不能 学会 ， 而 多层 神经网络 的 训练 仍然 看不到 希望 ， 这 导致 了 NN 的 第一个 冬天 。 " ,   Hinton   意识 到 ， 人工神经网络 不必 非 要 按照 生物 的 路子走 。 在 上 世纪 80 年代 ，   Hinton   和   LeCun   奠定 和 推广 了 可以 用来 训练 多层 神经网络 的 反向 传播 算法 ( back - propagation ) 。 NN 再次 迎来 了 春天 。 ,   反向 传播 算法 ， 说白了 就是 一套 快速 求 目标 函数 梯度 的 算法 。 ,   对于 最 基本 的 梯度 下降 ( Gradient   Descent ) ： ,     ， 反向 传播 就是 一种 高效 计算     的 方式 ,   不过 在 那时 ， NN 就 埋 下 了 祸根 。 ,   首先 是 ， 反向 传播 算法 在 生物学 上 很 难 成立 ， 很难 相信 神经系统 能够 自动 形成 与 正向 传播 对应 的 反向 传播 结构 （ 这 需要 精准 地求 导数 ， 对 矩阵 转置 ， 利用 链式法则 ， 并且 解剖学 上 从来 也 没有 发现 这样 的 系统 存在 的 证据 ） 。 反向 传播 算法 更 像是 仅仅 为了 训练 多层 NN 而 发展 的 算法 。 失去 了 生物学 支持 的 NN 无疑 少 了 很多 底气 ， 一旦 遇到 问题 ， 人们 完全 有 更 多 理由 抛弃 它 （ 历史 上 上 也 是 如此 ） ,   其次 是 ， 反向 传播 算法 需要 SGD 等 方式 进行 优化 ， 这 是 个 高度 非凸 的 问题 ， 其 数学 性质 是 堪忧 的 ， 而且 依赖 精细 调参 。 相比之下 ， （ 当时 的 ） 后起之秀 SVM 等等 使用 了 凸 优化 技术 ， 这些 都 是 让 人们 远离 NN 的 拉力 。 当 那 时候 的 人们 认为 DNN 的 训练 没有 希望 （ 当时 反向 传播 只能 训练 浅层 网络 ） 的 时候 ， NN 再次 走向 低谷 。 ,   深度 学习 时代 的 敲门砖 — — RBM ,   第二次 NN 低谷 期间 ， Hinton 没有 放弃 ， 转而 点 了 另外 一个 科技 树 ： 热力学 统计 模型 。 ,   Hinton 由 玻尔兹曼 统计 相关 的 知识 ， 结合 马尔科夫 随 机场 等 图 学习 理论 ， 为 神经网络 找到 了 一个 新 的 模型 ： 玻尔兹曼 机 ( BM ) 。 Hinton 用 能量 函数 来 描述 NN 的 一些 特性 ， 期望 这样 可以 带来 更 多 的 统计学 支持 。 ,   不久 Hinton 发现 ， 多层 神经网络 可以 被 描述 为 玻尔兹曼 机 的 一种 特例 — — 受限 玻尔兹曼 机 ( RBM ) 。 Hinton   在   Andrew   Ng   近期 对 他 的 采访 中   ( https : / / www . youtube . com / watch ? v = - eyhCTvrEtE ) ， 称其为   " most   beautiful   work   I   did " 。 ,   当年 我 第一次 看到   RBM   的 相关 数学 理论 的 时候 ， 真的 非常 激动 ， 觉得 这样 的 理论 不 work 有点 说不过去 。 这里 我 给出 相关 的 数学公式 ， 以 展示 NN 可以 有 完全 不同于 生物 的 诠释 方式 。 ,   在 统计力学 中 ， 玻尔兹曼 分布 （ 或称 吉布斯 分布 ） 可以 用来 描述 量子 体系 的 量子态 的 分布 ， 有着 以下 的 形式 ： ,   ,   其中   s   是 某个 量子态 ，     为 这个 状态 的 能量 ，     为 这个 状态 出现 的 概率 。 ,   k 是 玻尔兹曼 常量 ， 是 个 常数 。 T 是 系统 温度 ， 在 具体 问题 中 也 是 一个 常数 。 于是 我们 不妨 让 kT = 1 ， 原来 的 表达式 可以 简化 为 ,   ,   也 就是 ,   ,   这 不 就是   softmax   吗 ？ 居然 自然 地 在 统计力学 分布 里面 出现 了 （ 难怪 之前   LeCun   让 大家 学 物理 ） 。 ,   为了 再次 简化 ， 我们 定义     ， 于是 就 有 ,     ， （ 因为 这时候 公式 里面 只有 一个 s ， 就 没有 必要 写下 标了 ） ,   下面 问题 来 了 ，       是 什么 ？     又 应该 是 什么 ？ ,   Hinton   看 了 看 神经网络 的 一层 ， 其 分为 可见 层 （ 输入 层 ） 和 隐含 层 （ 中间层 ） 。 按照 经典 网络 的 定义 ， 神经元 有 激活 和 未激活 两个 状态 。 那么 干脆 让     等于 可见 层     并 上 隐含 层     神经元 的 状态 吧 （ 默认 都 用 向量 的 方式 表示 ） ： ,   于是     ， ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 3ac526ce73864d97fface2fea6ea3ca0 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 440 "   data - rawheight = " 468 "   class = " origin _ image   zh - lightbox - thumb "   width = " 440 "   data - original = " https : / / pic1 . zhimg . com / v2 - 3ac526ce73864d97fface2fea6ea3ca0 _ r . jpg " / > ,   （ RBM 示意图 ， 取自 Wikipedia ） ,   那么     又 是 什么 呢 ？ ,   非常 巧合 的 是 ， 量子 物理学 里面 有个 模型 极其 像 神经网络 ， 以至于 只要 了解 过 几乎 都 会 惊叹 两者 的 相似 度 。 这个 模型 就是 著名   易辛 模型 ( Ising   model ) 。 易辛 模型 （ 物理学界 常见 调侃 ： 你 3 维   Ising   模型 会解 了 吗 ？ ） 描述 了 晶格 系统 中 的 相变 ， 解释 了 铁磁性 问题 （ 你 可能 好奇 过 ， 为啥 这么 多 金属 ， 就 铁 等 少数 金属 特别 敏感 ， 而且 还 能 被 磁化 。 这个 模型 给出 了 解释 ） 。 ,   Hinton   把 神经元 的 偏置 ( 对于 可见 层 记作     ， 对于 隐含 层 记作     )   作为   Ising   model   的   “ 外场 ” ， NN 的 权重     作为   Ising   Model   的 “ 内部 耦合 系数 ” （ 两个 神经元 之间 的 权重 越大 ， 代表 它们 的 耦合 越强 ， 关联 越强 ） ， 于是 能量 就 可以 写作 非常简单 的 形式 ： ,   ,   这个 形式 让 人 惊讶 之 处 在于 ， 在 没有 浪费 任何 一个 NN 中 的 参量 的 情况 下 做到 了 最简 ， 并且 非常 合理 的 直觉 ： 神经元 的 偏置 只 和 神经元 本身 通过 乘法 直接 相关 ， 而 两个 神经元 间 的 权重 也 只 和 对应 的 两个 神经元 通过 乘法 直接 相关 ， 而 整体 的 贡献 用 加法 联系 起来 。 ,   我们 可以 将 某个 神经元     关联 的 能量 分离出来 ， 也 就是 ,     ， 其中     是 和 神经元     相连 的 权重 ，     是 除去     的 向量 。 ,   为了 方便 ， 我们 把 和     无关 的 部分 记 作 ,   ,   于是 ， ,   ,   于是 很 容易 得到 ,   ,   ,   这 不 就是 sigmoid 函数 吗 ？ 也 就是 ,   ,   这时候   sigmoid   函数 就 有 了 自然 的 解释 ： 玻尔兹曼 分布 下 隐含 层 神经元 激活 的 条件 概率 的 激活 函数 。 ,   如果 你 是   Hinton ， 推导 到 这 一步 ， 肯定 也 会 觉得 是 个 喜出望外 的 结果 吧 。 ,   而 优化 的 目标 ， 就是 极大 似然 估计 ， 也 就是 最大化 ,     ， 这里 其实 也 非常 有趣 ， 因为 和 热力学 统计 中 的 自由 能 非常 相关 。 ,   定义 自由 能为     （ “ 自由 ” 可以 理解 为     拥有 额外 的 自由度 ， 其 蕴含 的 能量 在 体系 中 可以 用来 对外 做功 ） ， 则 ,   于是 有     ， 即     是 关于 自由 能 的 玻尔兹曼 分布 。 也 就是 我们 找 的 参数 是 使得 出现 的 样本 的 自由 能 （ 在 参数 约束 的 分布 中 ） 最低 的 一组 参数 。 这样 参数 选择 就 和 样本分布 通过 最低 能量 联系 起来 。 ,   总之 一切 看上去 都 很 有 道理 。 Hinton 展现 了 NN 和 玻尔兹曼 分布 间 惊人 的 联系 （ 其 在 论文 中 多次 称   surprisingly   simple   [ 7 ] ） ， 其 背后 的 内涵 引 人 遐想 。 甚至 有人 在 听 过 Hinton 的 讲座 之后 ， 还 发现 RBM 的 训练 模式 和 量子 重整 化群 的 重整 化步骤 是 同构 的   [ 6 ] 。 ,   不过 问题 是 ， 优化 整体 网络 是 困难 的 ， 其 根源 性 被 认为 在于 配分函数     。 求得 最低 能量 对应 的 结构 一般 意义 上 是 个     的 问题 ， 如果 真的 能够 有 有效 算法 ， 那么 很多 热力学 系统 ， 包括   Ising   模型 也 就 迎刃而解 。 ,   Hinton   使用 贪心 的 方式 来 降低 算法 复杂度 ： 逐层 训练 网络 ， 而 不是 整体 优化 。 而 为了 训练 每层 RBM ， Hinton 发展 了 所谓 的 对比 散度 （ contrastive   divergence ） 算法 。 ,   CD 算法 利用 了   Gibbs   sampling ， 但是 算法 收敛 的 非常 慢 （ 这 已经 是 贪心 处理 过 的 问题 了 ， 可见 原 问题 多难 ） 。 Hinton 再次 近似 ， 固定 采样 步数     ， 被 称为     算法 。 Hinton   惊奇 的 发现     的 时候 （ 显然 是 极度 粗糙 的 近似 ） ， 算法 的 表现 就 已经 相当 良好 了 。 ,   Hinton   发现 用 这个 粗糙 的 算法 预 训练 网络 （ 这个 时候 是 无 监督 学习 ， 也 就是 只 需要 数据 ， 不 需要 标签 ； 在 下面 会 提到 ） 后 ， 就 可以 通过 调优 （ 加上 标签 ， 使用 反向 传播 继续 训练 ， 或者 干脆 直接 在 后面 接个 新 的 分类器 ） 高效 且 稳定 地 训练 深层 神经网络 。 ,   之后 “ 深度 学习 ” 这个 词 逐渐 走上 历史 的 前台 ， 虽然   1986 年 就 有 这个 概念 了   [ 8 ] 。 可以 说   RBM   是 这 一波 人工智能 浪潮 的 先行者 。 ,   这 让 人 想起 另外 一个 相当 粗糙 但是 甚至 更加 成功 的 算法 — — SGD 。 可以 说 ， 利用 梯度 的 算法   中 很 难 有 比 SGD 还 简单 的 了 ， 但是 SGD （ 加上 动量 后 ） 效果 确实 特别 好 。 非常 粗糙 的 算法 为何 却 对 NN 的 优化 这种 非常复杂 的 问题 很 有效 ， 这 仍然 是 一个 非常 有趣 的 开放 问题 。 ,   由于 玻尔兹曼 机 本身 的 特性 ， 其 可以 被 用来 解决 “ 无 监督 学习 ” （ Unsupervised   learning ） 相关 的 问题 。 即使 没有 标签 ， 网络 也 可以 自己 学会 一些 良好 的 表示 ， 比如 下面 是从 MNIST 数据 集中 学到 的 表示 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 90cb3b832018d5e5b4f32bc6e5520f5c _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 289 "   data - rawheight = " 289 "   class = " content _ image "   width = " 289 " / > ,   当 我们 将 人类 智能 ， 和 目前 的 人工 智障 对比 时 ， 常常 举 的 例子 就是 “ 现在 机器 学习 依赖 大 数据 ， 而 人类 的 学习 却是 相反 的 ， 依赖 小 数据 ” 。 这个 说法 其实 不尽 准确 。 人类 拥有 太多 的 感知 器官 ， 无时无刻 不 接收 着 巨量 的 数据 ： 就 按 人眼 的 分辨率 而言 ， 目前 几乎 没有 什么 实际 的 机器 学习 模型 模型 使用 如此 高清晰度 的 数据 进行 训练 的 。 我们 观察 一个 东西 的 时候 ， 所有 的 知觉 都 潜移默化 地 给 我们 灌输 海量 的 数据 ， 供 我们 学习 ， 推理 ， 判断 。 我们 所谓 的 “ 小 数据 ” ， 实际上 主要 分为 两个 部分 ： ,   少 标签 。 我们 遇到 的 “ 题目 ” 很多 ， 我们 无时无刻 不 在 接受 信息 ； 但是 我们 的 “ 答案 ” 很少 ， 我们 可能 看过 各种各样 的 人 ， 各种各样 的 动物 ， 直到 某 一天 才 有人 用 3 个 字 告诉 我们 ， “ 这 是 猫 ” 。 可能 一生 中 ， 别人 给 你 指出 这 是 猫 的 次数 ， 都 是 屈指可数 的 。 但是 ， 仅仅 通过 这 一两次 提示 （ 相当于 一两个 标签 ） ， 你 就 能 在 一生 中 记得 这些 概念 。 甚至 别人 从不 告诉 这 是 猫 ， 你 也 知道 这 应该 不是 狗 或者 其他 动物 。 这种 “ 没有 答案 ” 的 学习 称为   “ 无 监督 学习 ” （ Yann   LeCun 将 其 比作 蛋糕 胚 ， 以示 其 基础性 的 作用 ） ， 目前 机器 学习 在 无 监督 学习 方面 进展 很少 。 逻辑 推断 ， 因果 分析 。 也 可以 说 是 少 证据 。 如果 你 看过 探案 相关 的 小说 ， 那些 侦探 ， 能 从 非常 细微 的 证据 中 ， 得出 完整 的 逻辑 链 ； 现实 中 ， 爱因斯坦 等 物理学家 能够 从 非常少 的 几点 假设 构建 出 整套 物理学 框架 。 最早 的 人工智能 研究 很多 集中 在 类似 的 方面 （ 流派 被 称为 “ 符号 主义 ” ） ， 但是 事实证明 这些 研究 大多数 很难 应用 到 实际 问题 中 。 现在 NN 为 人 所 诟病 的 方面 之一 就是 很难 解决 逻辑 问题 ， 以及 因果 推断 相关 的 问题 （ 不过 最近 有些 进步 ， 比如 在 视觉 问答 VQA 方面 ） ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 5e2d71cee1a556cf2879d3e3ea3d922c _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 784 "   data - rawheight = " 502 "   class = " origin _ image   zh - lightbox - thumb "   width = " 784 "   data - original = " https : / / pic1 . zhimg . com / v2 - 5e2d71cee1a556cf2879d3e3ea3d922c _ r . jpg " / > ,   ( Yann   LeCun 的 蛋糕 ， 来自 网络 上 公开 的 Yann   LeCun   PPT 的 图片 ) ,   无 监督 学习 和 先验 知识 ,   这是 为了 帮助 理解 而 在 中间 插入 的 一 小节 。 这一 小节 强调 先验 知识 对 无 监督 学习 的 重要性 ， 这 有助于 理解 后面 为什么 Hinton 要 强行 把 所谓 “ 坐标 框架 ” 体现 在 模型 中 ， 因为 “ 坐标 框架 ” 就是 一种 先验 知识 ， 而且 是从 认知 神经科学 中 总结 的 先验 知识 。 ,   无 监督 学习 是 一种 没有 答案 的 学习 。 很 关键 的 一点 是 ， 没有 答案 怎么 学 ？ ,   子 曰 ： 学而不思 则 罔 ， 思而 不学则 殆 。 无 监督 学习 就 像 一个 “ 思而 不学 ” （ 这里 的 “ 学 ” 是 指 学习 书本 （ 即 较 直接 答案 ） ， 不是 指 广义 的 学习 ） 的 学生 。 显然 这个 学生 如果 没有 正确 的 思路 和 指导 方向 ， 自己 一直 凭空 想 下去 ， 八成 会 变成 一个 疯狂 级 的 黑暗 民科 。 ,   这个 “ 思路 和 指导 方向 ” 就是 我们 的 先验 知识 。 先验 知识 并 没有 限定 思考 的 范围 ， 但是 却 给出 了 一些 “ 建议 的 方向 ” 。 这 对 有 监督 和 无 监督 学习 都 很 重要 ， 但是 可能 对 无 监督 更加 关键 。 ,   我们 可以 回顾 一下 为什么 同 为 神经网络 ， CNN 在 图像 ， 甚至 语音 等 领域 全 方面 碾压 那种 “ 简单 ” 的 密 连接 网络 （ 参数 少 ， 训练 快 ， 得分 高 ， 易 迁移 ） ？ ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 5df9828848fecaaf323a64eb8249294f _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 634 "   data - rawheight = " 426 "   class = " origin _ image   zh - lightbox - thumb "   width = " 634 "   data - original = " https : / / pic4 . zhimg . com / v2 - 5df9828848fecaaf323a64eb8249294f _ r . jpg " / > ,   （ CNN 示意图 ， 来自 Wikipedia ） ,   显然 CNN 有 一个 很强 的 先验 关系 ： 局部性 。 它 非常 在意 局部 的 关系 ， 以及 从 局部 到 整体 的 过渡 。 ,   < img   src = " https : / / pic3 . zhimg . com / v2 - a4993e3f868d36baedeec0357bfeab0a _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1354 "   data - rawheight = " 1176 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1354 "   data - original = " https : / / pic3 . zhimg . com / v2 - a4993e3f868d36baedeec0357bfeab0a _ r . jpg " / > ,   （ AlphaGo 中 使用 CNN 提取 围棋 的 特征 ， 取自   DeepMind   关于   AlphaGo 的 论文   ） ,   这 在 围棋 中 也 非常明显 ， 使用 CNN 的 AlphaGo 能够 “ 看清 ” 局部 的 关系 ， 同时 能够 有 很 好 的 大局观 。 ,   而换 一个 领域 ， Kaggle   比如 上面 表格 数据 的 学习 ， CNN 就 差 多 了 ， 这时候 胜出 往往 是 各种 集成 方法 ， 比如   Gradient   Boosting   和   Random   Forest 。 因为 这些 数据 很少 有 局部 关联 。 ,   无 监督 领域 比较 成熟 的 算法 大多 是 聚类 算法 ， 比如   k - Means   等等 。 ,   这些 算法 聚类 显著 的 特点 是 强调 空间 相关 的 先验 ， 认为 比较 靠近 的 是 一类 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 7fdc51f60de15b36377ff29b6788b7cc _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1404 "   data - rawheight = " 790 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1404 "   data - original = " https : / / pic1 . zhimg . com / v2 - 7fdc51f60de15b36377ff29b6788b7cc _ r . jpg " / > ,   ( 图为 两个 不同 的 聚类 算法 的 效果 ， 取自 Wikipedia   k - Means 页面 ) ,   然而 即使如此 ， 两个 聚类 算法 的 不同 的 先验 知识 仍然 导致 不同 的 结果 。 上面 图中 ， k - Means 的 先验 更 强调 cluster 的 大小 均匀 性 （ 损失 是 聚类 中心 到 类 成员 的 距离 平方 ） ， 因此 有 大 而 平均 的 聚类 簇 ； 而 高斯 EM 聚类 则 更 强调 密集 性 （ 损失 是 中心 到 成员 的 距离 的 指数 ） ， 因此 有 大小不一 但是 密集 的 聚类 簇 。 （ 大多数 人 更加 偏向 EM 的 结果 ， 这 大多 是因为 我们 对 米老鼠 的 ， 或者 对 动物 头部 的 先验 知识 ， 希望 能够 分出 “ 耳朵 ” 和 “ 脸 ” ） ,   人 的 先验 知识 是 我们 最 关心 的 ， 这 可能 是 AI 的 核心 。 比如 下面 的   " tufa "   问题 。 我们 随便 指出 一个 人们 从来 没有 看过 的 图案   “ tufa ” ， 然后 让 人们 指出 剩下 哪些 图案 是   " tufa " 。 人们 成功率 会 很 高 。 而 这个 问题 （ one - shot   learning ） 对 机器 却 很 难 。 ,   < img   src = " https : / / pic3 . zhimg . com / v2 - 92d84486626ce60c3151bf37c992daba _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1712 "   data - rawheight = " 1296 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1712 "   data - original = " https : / / pic3 . zhimg . com / v2 - 92d84486626ce60c3151bf37c992daba _ r . jpg " / > ,   ( 图片 来源 ： University   of   Oxford ,   Lecture1   Introduction ,   Nando   de   Freitas ) ,   这 似乎 是 一种 天然 的 能力 。 很难 相信 没有 先验 知识 的 机器 能 做到 这件 事 。 ,   另外 ， 人 和 动物 的 视觉 系统 有着 异常 复杂 的 ， 现今 仍然 没有 完全 搞清楚 的 内部结构 ， 这种 特异 化 的 结构 同样 是 先验 知识 的 有力 证据 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - c82809f6986acd30f25dde19a8cafb28 _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 992 "   data - rawheight = " 1268 "   class = " origin _ image   zh - lightbox - thumb "   width = " 992 "   data - original = " https : / / pic1 . zhimg . com / v2 - c82809f6986acd30f25dde19a8cafb28 _ r . jpg " / > ,   ( 猴子 的 视觉 系统 各个 部分 的 关联 ， Felleman   &   Van   Essen   1991 ) ,   近期 有 不少 RL （ 强化 学习 ） 方面 的 论文 试图 探究 和 模仿 人 的 先验 知识 。 比如 下面 的 这篇 论文 试图 建模 关于 “ 好奇心 ” 的 先验 知识 ， 鼓励 模型 自己 探究 特殊 之 处 ， 还是 有 一些 奇效 的 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 26883b9ff5a865941d6e04a9097f636c _ b . jpg "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1530 "   data - rawheight = " 1190 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1530 "   data - original = " https : / / pic1 . zhimg . com / v2 - 26883b9ff5a865941d6e04a9097f636c _ r . jpg " / > ,   ( 图片 取自 论文   Curiosity - driven   Exploration   by   Self - supervised   Prediction ) ,   后面 我们 会 看到   Hinton   通过 认知科学 和 图形学 总结 出来 的 一些 先验 知识 ， 以及 他 如何 将 这些 先验 知识 加入 到 模型 中 去 。 ,   反向 传播 ， 它 就是 有效 ,   不过 不久 ， 人们 发现 ， 使用 ReLU 以及 合适 的 初始化 方法 ， 用 上 CNN ， 搭配 上 强劲 的 GPU 之后 ， 发现 原来 的 深度 神经网络 可以 照常 训练 ， 根本 不用 RBM 预 训练 。 RBM 虽然 数学 上 很漂亮 ， 但是 受 结构限制 严重 ， 而且 在 supervised   learning 方面 往往 搞 不过 直接 暴力 反向 传播 。 前 几年 Andrew   Y .   Ng 在 Google 让 神经网络 自动检测 视频 中 的 猫 的 时候 ， Google 内部 的 深度 学习 框架 几乎 就是 用来 支持 RBM 等 的 训练 的 。 而 现在 Google 开源 的 TensorFlow 等 主流 框架 中 都 没有 RBM 的 影子 。 很多 从 TensorFlow 入手 的 新人 估计 也 没有 听过 RBM 。 ,   好 了 ， 现在 除了 各种 小修 小改 （ 残差 网络 ， Adam 优化 器 ， ReLU ， Batchnorm ， Dropout ， GRU ， 和 稍微 创意 点 的 GAN ） ， 神经网络 训练 主流 算法 又 回到 了 30 年前 （ 那个 时候 CNN ， LSTM 已经 有 了 ） 的 反向 传播 了 。 ,   目前 来看 ， 很多 对   NN   的 贡献 （ 特别 是 核心 的 贡献 ） ， 都 在于 NN 的 梯度 流 上 ， 比如 ,   sigmoid 会 饱和 ， 造成 梯度 消失 。 于是 有 了 ReLU 。 ReLU 负 半轴 是 死区 ， 造成 梯度 变 0 。 于是 有 了 LeakyReLU ， PReLU 。 强调 梯度 和 权值 分布 的 稳定性 ， 由此 有 了 ELU ， 以及 较 新 的 SELU 。 太深 了 ， 梯度 传 不 下去 ， 于是 有 了 highway 。 干脆 连 highway 的 参数 都 不要 ， 直接 变 残差 ， 于是 有 了 ResNet 。 强行 稳定 参数 的 均值 和 方差 ， 于是 有 了 BatchNorm 。 在 梯度 流中 增加 噪声 ， 于是 有 了   Dropout 。 RNN 梯度 不 稳定 ， 于是 加 几个 通路 和 门控 ， 于是 有 了 LSTM 。 LSTM 简化 一下 ， 有 了 GRU 。 GAN 的 JS 散度 有 问题 ， 会 导致 梯度 消失 或 无效 ， 于是 有 了 WGAN 。 WGAN 对 梯度 的 clip 有 问题 ， 于是 有 了 WGAN - GP 。 ,   说到底 ， 相对 于 8 ， 90 年代 （ 已经 有 了 CNN ， LSTM ， 以及 反向 传播 算法 ） ， 没有 特别 本质 的 改变 。 ,   但是 为什么 当前 这种 方式 实际效果 很 好 ？ 我 想 主要 有 ： ,   全 参数 优化 ， end - to - end 。 反向 传播 （ 下面 用 BP 代替 ） 可以 同时 优化 所有 的 参数 ， 而 不 像 一些 逐层 优化 的 算法 ， 下层 的 优化 不 依赖 上层 ， 为了 充分利用 所有权 值 ， 所以 最终 还是 要用 BP 来 fine - tuning ； 也 不 像 随机 森林 等 集成 算法 ， 有 相对 分立 的 参数 。 很多 论文 都 显示 end - to - end 的 系统 效果 会 更好 。 形状 灵活 。 几乎 什么 形状 的 NN 都 可以 用 BP 训练 ， 可以 搞 CNN ， 可以 搞 LSTM ， 可以 变成 双向 的   Bi - LSTM ， 可以 加 Attention ， 可以 加 残差 ， 可以 做成 DCGAN 那种 金字塔 形 的 ， 或者 搞出 Inception 那种 复杂 的 结构 。 如果 某个 结构 对 NN 很 有利 ， 那么 就 可以 随便 加进去 ； 将 训练 好 的 部分 加入 到 另 一个 NN 中 也 是 非常 方便 的 事情 。 这样 随着 时间 推进 ， NN 结构 会 被 人工 优化 得 越来越 好 。 BP 的 要求 非常低 ： 只要 连续 ， 就 可以 像 一根 导线 一样 传递 梯度 ； 即使 不 连续 ， 大部分 也 可以 归结为 离散 的 强化 学习 问题 来 提供 Loss 。 这 也 导致 了 大量 NN 框架 的 诞生 ， 因为 框架 制作者 知道 ， 这些 框架 可以 用于 所有 需要 计算 图 的 问题 （ 就 像 万能 引擎 ） ， 应用 非常 广泛 ， 大部分 问题 都 可以 在 框架 内部 解决 ， 所以 有 必要 制作 。 计算 高效 。 BP 要求 的 计算 绝大多数 都 是 张量 操作 ， GPU 跑 起来 贼 快 ， 并且 NN 的 计算 图 的 形式 天生 适合 分布式计算 ； 而且 有 大量 的 开源 框架 以及 大 公司 的 支持 。 ,   神经 解剖学 与   Capsule   的 由 来 ,   不过   Hinton   看上去 是 不会 对 目前 这种 结果 满意 的 。 他 在 2011 年 的 时候 ， 就 第一次 提出 了 Capsule   结构 [ 9 ] （ 我们 会 在 后面 解释 Capsule 是 什么 ） 。 不过 那次 Hinton 打擂 显然 没有 成功 。 ,   Hinton 最近 抓住 了 NN 中 最 成功 的 CNN 批判 了 一番 ， 又 重新 提出 了 Capsule   结构 。 可以 明确 的 是 ， Hinton   受到 了 下面 3 个 领域 的 启示 ： ,   神经 解剖学 认知 神经科学 计算机 图形学 ,   其中 前 两者 明显 是 和 人脑 相关 的 。 可能 不少 读者 都 有 疑问 ： NN 非要 按照 生物 的 路子走 吗 ？ ,   回答 是 ： 看 情况 。 ,   对于 人脑 中 存在 的 结构 和 现象 ， 可以 从 不同 的 观点 看待 ： ,   这是 生物 基础 导致 的 妥协 ， 是 进化 的 累赘 。 由于 细胞 构成 的 生物 系统 难以完成 某些 特定 任务 ， 而 以 实质 上 非常 低效 的 方式 勉强 实现 。 这时候 不 模仿 人脑 是 正确 的 。 典型 的 例子 是 算术 计算 以及 数据 存储 。 生物 结构 很难 进化 出 精确 的 运算 元件 ， 以及 大容量 的 存储元件 ， 并且 让 它们 能以 GHz 量级 的 频率 持续 工作 。 我们 只能 用 高层 的 、 抽象 的 方式 进行 不 保证 精准 的 运算 、 记忆 ， 这 大大 慢于 当代 的 计算机 ， 也 没有 计算机 准确 。 比如 知乎 上 这个 问题   比特 币 挖矿 一定 要 用 计算机 吗 ？ 用纸 笔来 计算 可行 吗 ？ ， 有 很多 折叠 的 回答 是 “ 这 孩子 能 用来 做 显卡 ” 。 虽然 这些 回答 有 侵犯 性 ， 但是 确实 足以 说明 这些 方面 生物 结构 的 显著 弱势 。 这是 演化 中 的 中 性功能 。 进化 只 要求 “ 够用 ” ， 而 不是 “ 最好 ” 。 有些 人脑 的 结构 和 功能 也许 可以 被 完全 不同 的 实现 方式 替代 。 这里 的 一个 例子 是   AlphaGo   下围棋 。 围棋 高手 能够 把 围棋 下 的 很 好 ， 但是 普通人 不能 。 下围棋 确乎 关系 到 人 的 直觉 ， 但是 这种 直觉 不是 强制 的 ， 也 不是 先天 的 ： 不会 下围棋 不 意味着 会 在 进化 中 淘汰 ， 人脑 中 也 没有 专用 的 “ 围棋 模块 ” 。 这个 时候 ， 我们 可以 设计 一个 和 人 脑机制 差异 很大 的 系统 ， 比如 AlphaGo ， 它 可以 下得 比人 还要 好 。 这是 演化 中 的 重大突破 ， 这些 功能 造就 了 我们 “ 人 ” 的 存在 。 比如 人 的 各类 感知 系统 ， 人 的 因果 分析 系统 ， 学习 系统 ， 规划系统 ， 运动 控制系统 。 这些 是 人工智能 尚且 欠缺 的 。 ,   不过 首要 问题 是 ， 我们 怎么 知道 某个 人脑 的 功能 或者 结构 属于 上面 的 第 3 点 呢 ？ 按照 上面 的 观点 ， 显然 生物 的 某个 结构 和 功能 本身 的 出现 不能 说明 它 很 有用 。 我们 需要 更 多 证据 。 ,   一个 重要 的 统计学 证据 是 普遍性 。 我们 为什么 会 有 拿 NN 做 AI 的 想法 ？ 因为 NN 本身 正是 生物进化 中 的 重大突破 ， 凡是 有 NN 的 生物 中 ， 我们 都 发现 NN 对 其 行为 调控 起 了 关键性 作用 ， 尤其 是 人类 。 这 也 是 我们 如今 愿意 相信 它 的 理由 ， 而 不 只是 因为 人有 一个 大脑 ， 所以 我们 就 必须 搞 一个 （ 就 像 我们 不 给 AI 做 肝脏 一样 ） 。 ,   人 的 实际 神经系统 是 有 分层 的 （ 比如 视觉 系统 有 V1 ,   V2 等等 分层 ） ， 但是 层数 不 可能 像 现在 的 大型 神经网络 （ 特别 是 ResNet 之后 ） 一样 动不动 就 成百上千 层 （ 而且 生物学 上 也 不 支持 如此 ， 神经 传导 速度 很 慢 ， 不像 用 GPU 计算 神经网络 一层 可能 在 微秒 量级 ， 生物 系统 传导 一次 一般 在 ms 量级 ， 这么 多层 数 不 可能 支持 我们 现在 这样 的 反应速度 ， 并且 同步 也 存在 问题 ） 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - fda61f26058bc41257d733554eaffb20 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1200 "   data - rawheight = " 800 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1200 "   data - original = " https : / / pic1 . zhimg . com / v2 - fda61f26058bc41257d733554eaffb20 _ r . jpg " / > ,   ( 将 人脑 视觉 通路 分层 和 DNN 分层 的 类比 。 Image   ( c )   Jonas   Kubilias ) ,   Hinton   注意 到 的 一个 有趣 的 事实 是 ， 目前 大多数 神经 解剖学 研究 都 支持 （ 大部分 哺乳类 ， 特别 是 灵长类 ） 大脑皮层 中 大量 存在 称为   Cortical   minicolumn   的 柱状 结构 （ 皮层 微柱 ） ， 其 内部 含有 上 百个 神经元 ， 并 存在 内部 分层 。 这 意味着 人脑 中 的 一层 并 不是 类似 现在 NN 的 一层 ， 而是 有 复杂 的 内部结构 。 ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 17c4bd8ecacd3265ecee9cc024413867 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 896 "   data - rawheight = " 720 "   class = " origin _ image   zh - lightbox - thumb "   width = " 896 "   data - original = " https : / / pic4 . zhimg . com / v2 - 17c4bd8ecacd3265ecee9cc024413867 _ r . jpg " / > ,   （ mini - column   图片 ， 引 自   minicolumn   hypothesis   in   neuroscience   |   Brain   |   Oxford   Academic ） ,   为什么 大脑皮层 中 普遍存在   mini - column ？ 这 显然 是 一个 重要 的 统计学 证据 ， 让   Hinton   愿意 相信   mini - column   肯定 起 了 什么 作用 。 于是   Hinton   也 提出 了 一个 对应 的 结构 ， 称为   capsule   （ 胶囊 ， 和 微柱 对应 ） 。 这 就是   capsule   的 由 来 。 ,   但是   capsule   做 了 什么 ？ 之前 的 CNN 又 有 什么 问题 ？ 统计学 证据 不能 给出 这些 的 答案 。 Hinton 的 这部分 答案 来自 认知 神经科学 。 ,   认知 神经科学 和 “ 没有 免费 的 午餐 ” ,   每 一个 机器 学习 的 初学者 都 应该 了解 关于 机器 学习 的 重要 定律 — — “ 没有 免费 的 午餐 ” [ 10 ] ,   这个 可以 通过 科幻小说 《 三体 》 里面 的 提到 一个 例子 来 理解 ： ,   “ 农场主 假说 ” 则 有 一层 令人不安 的 恐怖 色彩 ： 一个 农场 里 有 一群 火鸡 ， 农场主 每天 中午 十一点 来 给 它们 喂食 。 火鸡 中 的 一名 科学家 观察 这个 现象 ， 一直 观察 了 近 一年 都 没有 例外 ， 于是 它 也 发现 了 自己 宇宙 中 的 伟大 定律 ： “ 每天 上午 十一点 ， 就 有 食物 降临 。 ” 它 在 感恩节 早晨 向 火鸡 们 公布 了 这个 定律 ， 但 这天 上午 十一点 食物 没有 降临 ， 农场主 进来 把 它们 都 捉 去 杀 了 。 ,   在 这个 例子 中 ， 问题 是 ， 火鸡 愚蠢 吗 ？ ,   观点 1 ： 火鸡 很 聪明 。 它 能够 发现 和 总结 规律 。 只不过 它 在 农场 很 不 走运 。 观点 2 ： 火鸡 很 愚蠢 。 无论如何 ， 它 没有 能够 让 自己 逃脱 死亡 的 命运 。 而且 正是 它 自己 得到 的 “ 规律 ” 将 它们 送上 死亡 之 路 。 ,   观点 2 就是 “ 没有 免费 的 午餐 ” 。 这 是 在 “ 数学 现实 ” 中 成立 的 ， 在 “ 数学 现实 ” 中 ， 一切 可能性 都 存在 ， 感恩节 那天 ， 火鸡 有 可能 被 杀 ， 也 有 可能 被 农场主 的 孩子 当成 宠物 ， 也 有 可能 农场主 决定 把 一部分 鸡 再养 一年 然后 杀掉 。 鸡 无论 做出 怎样 的 猜想 都 可能 落空 。 可以 证明 ， 无论 我们 学习 到 了 什么 东西 ， 或者 掌握 到 了 什么 规律 ， 我们 总是 可以 （ 在 数学 上 ） 构造 一个 反例 （ 比如 ， 让 太阳 从 西边 升起 ， 让 黄金 变成 泥土 ） ， 与 我们 的 判断 不 一致 。 这 不管 对于 机器 ， 而是 对于 人 ， 都 是 一样 的 。 也 就是 在 “ 一般 “ 的 意义 上 ， 或者 数学 的 意义 上 ， 没有 哪个 生物 ， 或者 哪个 算法 ， 在 预测 能力 上 比 瞎猜 更好 。 ,   而 看似 矛盾 的 观点 1 ， 却 在 物理 现实 中 得以 成立 。 可以 说 ， 物理 定律 是 一部分 不能 用 数学 证明 的 真理 。 我们 相信 这些 定律 ， 一 是因为 我们 尚且 没有 发现 违背 的 情况 ， 二是 某种 直觉 告诉 我们 它 很 可能 是 对 的 。 为什么 我们 能 总结 出 这些 定律 ， 这是 一个 让 人 困惑 的 问题 ， 因为 看起来 人 并 不是 先天 就 能 总结 出 各种 定律 。 但是 可以 确定 的 是 ， 我们 本身 就是 定律 约束 下 进化 的 产物 ， 虽然 对 物理 定律 的 理解 不是 我们 的 本能 ， 但是 很多 “ 准 定律 ” 已然 成为 我们 的 本能 ， 它们 塑造 了 我们 本能 的 思考问题 的 方式 ， 对 对称性 的 理解 ， 等等等等 。 ,   现实 中 的 情况 介于 观点 1 和 观点 2 之间 。 很多 东西 既 不是 完全 没有 规律 ， 也 不是 一种 物理 定律 ， 但是 对 我们 的 进化 和 存活 意义 重大 （ 也 就是 上面 说 的 “ 准 定律 ” ） ， 它们 是 一种 非常 强 的 “ 先验 分布 ” ， 或者说 ， 是 我们 的 常识 ， 而且 我们 通常 情况 下意识 不到 这种 常识 。 ,   既然 不是 物理 定律 ， 那么 按照 观点 2 ， 我们 就 能够 找到 一些 反例 。 这些 反例 对 我们 来说 是 某种 “ 错误 ” ， 这种 错误 正是 非常 非常 强 的 证据 。 理由 是 ， 我们 很少 出错 （ 指 认知 和 脑 功能 上 的 出错 ） 。 人脑 是 个 黑盒 ， 在 绝大多数 时候 都 工作 正常 ， 我们 从中 获得 的 信息量 很小 。 但是 一旦 出错 ， 就 能 给予 我们 很大 的 信息量 ， 因为 我们 得以 有 机会 观察 到 一些 奇特 的 现象 ， 好似 百年一遇 的 日全食 一般 。 很多 神经科学 上面 的 发现 都 建立 在 错误 之上 （ 比如 脑损伤 导致 了 语言 区 的 发现 ， 以及 左右脑 功能 的 确认 等等 ） 。 它 揭示 了 一些 我们 的 本能 ， 或者 我们 习得 的 先验 知识 。 ,   根据 上文 所述 ， 这种 先验 知识 ， 对于 机器 学习 ， 尤其 是 无 监督 学习 ， 是 极度 重要 的 。 ,   而 认知 神经科学 就 可以 通过 一些 实验 揭示 出 这些 错误 。 下面 给出 一些 例子 ： ,   第一个 例子 是 下面 的 人脸 ： ,   < img   src = " https : / / pic2 . zhimg . com / v2 - 8fb581b999325d5fac12e4cd1fdd7f15 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 600 "   data - rawheight = " 656 "   class = " origin _ image   zh - lightbox - thumb "   width = " 600 "   data - original = " https : / / pic2 . zhimg . com / v2 - 8fb581b999325d5fac12e4cd1fdd7f15 _ r . jpg " / > ,   这个 人 是 什么样 的 表情 ？ 倒 过来 再 看看 ？ ,   这个 例子 说明 了 人 对 倒 过来 的 人脸 的 表情 的 识别 能力 很差 。 长期 的 进化 过程 中 ， 我们 对 正 着 的 人脸 造成 了 “ 过 拟合 ” ， “ 正 着 ” 的 信息 变得 不是 很 重要 。 上面 的 图 出现 错觉 的 原因 是 ， 虽然 人脸 是 倒 着 的 ， 我们 却 用 “ 正 着 ” 的 思路 观察 图片 中 眼睛 ， 而 眼睛 的 线条 走向 给 了 我们 表情 信息 ： ,   （ 甚至 一些 简单 的 线条 ， 都 会 让 我们 觉得 是 人脸 ， 并且 得出 它 的 表情 。 其中 眼睛 和 嘴 的 线条 在 我们 表情 识别 中起 了 重要 作用 ） ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 96855d434ef5c872544683dd0ad9226b _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 910 "   data - rawheight = " 782 "   class = " origin _ image   zh - lightbox - thumb "   width = " 910 "   data - original = " https : / / pic4 . zhimg . com / v2 - 96855d434ef5c872544683dd0ad9226b _ r . jpg " / > ,   这 启示 我们 ， 人类 识别 脸 ， 其实 就是 通过 几个 关键 的 结构 （ 眼睛 ， 眉毛 ， 嘴 ， 鼻子 ） 完成 的 。 当今 很多 算法 都 模仿 这 一点 ， 标注 出 人脸 的 关键 结构 ， 成功率 很 高 。 ,   另外 人 对 脸 的 形状 过 拟合 ， 也 让 我们 看 二次元 中 动画人物 的 脸时 觉得 很 正常 ， 实际上 这 和 真实 的 脸 差异 很大 ， 但是 我们 大脑 不 这么 认为 ， 因为 这种 识别 机制 已经 成为 了 我们 的 本能 。 ,   第二个 例子 是 这个 错觉 图 ： ,   < img   src = " https : / / pic3 . zhimg . com / v2 - c5a44297ed3dbe30913fbeadefc39a4a _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 992 "   data - rawheight = " 768 "   class = " origin _ image   zh - lightbox - thumb "   width = " 992 "   data - original = " https : / / pic3 . zhimg . com / v2 - c5a44297ed3dbe30913fbeadefc39a4a _ r . jpg " / > ,   ( 图片 取自   Wikipedia ) ,   很难 想象 ， A 和 B 的 颜色 居然 是 一样 的 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 95cd972bc561ddfa045f143c740fb1e0 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 992 "   data - rawheight = " 768 "   class = " origin _ image   zh - lightbox - thumb "   width = " 992 "   data - original = " https : / / pic1 . zhimg . com / v2 - 95cd972bc561ddfa045f143c740fb1e0 _ r . jpg " / > ,   造成 这个 错觉 的 原因 是 ， 对 了 对应 自然界 中 阴影 对 颜色 识别 的 副作用 ， 我们 大脑 擅自 “ 减去 ” 了 阴影 对 颜色 的 影响 。 在 进化 中 ， 我们 正如 火鸡 一样 ， 觉得 “ 每天 上午 十一点 ， 就 有 食物 降临 ” ； 同样 的 ， 我们 觉得 “ 把 阴影 对 颜色 的 干扰 消除 掉 ， 就 能 识别 得 更好 ” ， 这 成为 了 我们 的 “ 准 定律 ” 。 然而 ， 上面 的 错觉 图中 ， 要求 我们 比较 A 和 B 的 颜色 ， 就 好似 感恩节 对 火鸡 一样 ， 我们 大脑 仍然 不听话 地 擅自改变 颜色 ， 导致 我们 在 这个 极其 特殊 的 问题 上 判断 失误 。 只不过 这个 失误 不 导致 什么 后果 罢了 ， 当然 如果 外星人 打算 利用 这个 失误 作为 我们 的 弱点 来 对付 我们 ， 那 就是 另外 一种 剧情 。 ,   下面 这个 图片 是 更加 极端 的 情况 。 中间 的 条带 其实 没有 渐变 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 517298c9e2e71f5b3c2163d30f93ca78 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1024 "   data - rawheight = " 744 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1024 "   data - original = " https : / / pic1 . zhimg . com / v2 - 517298c9e2e71f5b3c2163d30f93ca78 _ r . jpg " / > ,   ,   第三个 错觉 是 关于 线条 的 ： ,   < img   src = " https : / / pic3 . zhimg . com / v2 - 6e4df240a745d449af3f286e656ada56 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 648 "   data - rawheight = " 690 "   class = " origin _ image   zh - lightbox - thumb "   width = " 648 "   data - original = " https : / / pic3 . zhimg . com / v2 - 6e4df240a745d449af3f286e656ada56 _ r . jpg " / > ,   人类 会 不由自主 的 觉得 中间 似乎 有个 白色 三角形 ， 因为 我们 大脑 “ 骗 ” 我们 ， 让 我们 觉得 似乎 有 一些 “ 看不见 的 边 ” 。 ,   把 效果 变得 更 夸张 一点 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 5fce44ab4c333cd015aa89e4adcf0d74 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 768 "   data - rawheight = " 768 "   class = " origin _ image   zh - lightbox - thumb "   width = " 768 "   data - original = " https : / / pic1 . zhimg . com / v2 - 5fce44ab4c333cd015aa89e4adcf0d74 _ r . jpg " / > ,   按照 一定 距离 观察 这幅 图 ， 会 让 我们 觉得 产生 了 “ 缠绕 ” 或者 “ 扭曲 ” 。 实际上 这些 就是 一个个 同心圆 。 产生 错觉 的 原因 是 ， 大脑 给 我们 “ 脑补 ” 了 很多 倾斜 边 （ 这些 方块 是 倾斜 的 ， 并且 采用 了 不同 的 颜色 加强 边 的 效果 ） ， 这些 边 的 形状 不同于 它们 的 排列 方向 ， 因此 会 觉得 “ 缠绕 ” 。 如果 我们 到 了 这样 的 图案 居多 的 世界 中 ， 我们 的 现在 视觉 系统 将 难以 正常 工作 。 ,   我们 生活 中 的 绝大多数 物体 ， 都 有着 明确 的 边界 。 这 不是 一个 物理 定律 ， 但是 就 其 普遍性 而言 ， 足够 成为 一个 “ 准 定律 ” 。 以至于 人 和 动物 的 大脑 视觉 皮层 拥有 专门 识别 边 的 结构 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 02b64fe89dc1e02221496ab668772f38 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 964 "   data - rawheight = " 698 "   class = " origin _ image   zh - lightbox - thumb "   width = " 964 "   data - original = " https : / / pic1 . zhimg . com / v2 - 02b64fe89dc1e02221496ab668772f38 _ r . jpg " / > ,   CNN   被 认为 在 生物学 上 收到 支持 的 原因 之一 ， 在于 能够 通过 学习 自动 得到 边缘 等 特征 的 filter （ 非常 像 所谓 的   Gabor   filter ） : ,   < img   src = " https : / / pic3 . zhimg . com / v2 - 0791f1c3f6f9ee445ad1895ef0cc4daa _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 697 "   data - rawheight = " 348 "   class = " origin _ image   zh - lightbox - thumb "   width = " 697 "   data - original = " https : / / pic3 . zhimg . com / v2 - 0791f1c3f6f9ee445ad1895ef0cc4daa _ r . jpg " / > ,   CNN 成功 之 处 在于 能够 非常 成功 的 抽取 到 图像 的 特征 。 这 在   Neural   Style   项目 的 风格 迁移 （ 原图 + 风格 - > 带 风格 的 原图 ） 中 表现 得 非常 好 ： ,   < img   src = " https : / / pic2 . zhimg . com / v2 - 3bbde769dad7d8f35f78ef7bb1eed7cd _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1024 "   data - rawheight = " 683 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1024 "   data - original = " https : / / pic2 . zhimg . com / v2 - 3bbde769dad7d8f35f78ef7bb1eed7cd _ r . jpg " / > ,   人类 的 这些 错觉 同时 也 暗示 了 人类 和 算法 模型 一样 受 “ 没有 免费 午餐 定理 ” 的 限制 ， 人 的 认知 并 没有 特别 异 于 算法 的 地方 ， 或许 是 可以 被 算法 复现 的 。 ,   Hinton   从 认知 神经科学 中 得到 的 反对 CNN 的 理由 ,   说   Hinton   是 一个 认知 神经 科学家 并 没有 问题 。 Hinton 做过 不少 认知 实验 ， 也 在 认知科学 领域 发 过 不少 论文 。 ,   Hinton 自己 也 承认 ， CNN 做 的 非常 好 。 但是 当 Hinton 做 了 一系列 认知 神经科学 的 试验 后 ， HInton   觉得 有些 动摇 ， 直至 他 现在 反对 CNN 。 ,   第一个 实验 称为 四面体 谜题 （ tetrahedron   puzzle ） ， 也 是   Hinton   认为 最有 说服力 的 实验 。 ,   如图 ， 有 两个 全等 的 简单 积木 ， 要求 你 把 它们 拼成 一个 正四面体 （ 不要 看 答案 ， 先 自己 试试 ） 。 ,   < img   src = " https : / / pic3 . zhimg . com / v2 - 92ad6f160b3834e705c5d92c00a1a61e _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1175 "   data - rawheight = " 503 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1175 "   data - original = " https : / / pic3 . zhimg . com / v2 - 92ad6f160b3834e705c5d92c00a1a61e _ r . jpg " / > ,   这 理应 是 个 非常 非常简单 的 问题 ， 对于 类似 的 问题 ， 人们 平均 能 在 5 秒 内 解决 。 但是 Hinton   惊讶 的 发现 ， 对于 这个 问题 人们 平均 解决 的 时间 超乎 意料 的 长 ， 往往 要 几十秒 甚至 几分钟 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 22d4272657b29e7cf3bb95b94988a914 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1272 "   data - rawheight = " 950 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1272 "   data - original = " https : / / pic1 . zhimg . com / v2 - 22d4272657b29e7cf3bb95b94988a914 _ r . jpg " / > ,   （ 视频 中 Hinton 亲自 演示 这个 实验 的 样子 很 有趣 ， 取自 Youtube [ 2 ] ） ,   Hinton   此处 狂黑 MIT ， 说 MIT 教授 解决 这个 问题 的 分钟 数 和 和 他们 在 MIT 工作 的 年数 基本一致 ， 甚至 有 一个 MIT 教授 看来 半天 写 了 一个 证明 了 说 这 是 不 可能 的 （ 然后 底下 MIT 的 学生 听 了 非常高兴 ， 他们 很 喜欢 黑 自己 的 教授 ） 。 ,   但是 两类 人 解决 得 非常 快 ， 一类 是 本来 就 对 四面体 的 构型 非常 了解 的 ； 另外 就是 不 认真对待 随便 瞎试 的 （ 毕竟 就 几种 可能 情况 ， 枚举 起来 很快 ） 。 但是 如果 希望 通过观察 ， 通过 视觉 直觉 解决问题 会 非常 困难 。 ,   这 意味着 我们 出现 了 错觉 ， 而且 是 一种 视觉 错觉 。 ,   Hinton   通过 人们 尝试 的 过程 发现 ， 错觉 是 由于 人们 不 自觉 地会 根据 物体 形状 建立 一种 “ 坐标 框架 ” ( coordinate   frame ) ,   < img   src = " https : / / pic1 . zhimg . com / v2 - e7104df933c9b828da3fe8d8f3d59edc _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1160 "   data - rawheight = " 762 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1160 "   data - original = " https : / / pic1 . zhimg . com / v2 - e7104df933c9b828da3fe8d8f3d59edc _ r . jpg " / > ,   人们 会 不 自主 地 给 两个 全等 的 几何体 使用 相同 的 坐标 框架 。 这个 坐标 框架 会 造成 误导 ， 导致 人们 总是 先 尝试 一些 错误 的 解 。 ,   如果 给 两个 几何体 不同 的 坐标 框架 ,   < img   src = " https : / / pic2 . zhimg . com / v2 - d5250e2ab5cb2216f3039c9b0e0883ed _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1036 "   data - rawheight = " 714 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1036 "   data - original = " https : / / pic2 . zhimg . com / v2 - d5250e2ab5cb2216f3039c9b0e0883ed _ r . jpg " / > ,   几乎 就 立即 可以 得到 解 ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 4f6462db41dbf60bdd5d232211d0cf7b _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1084 "   data - rawheight = " 736 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1084 "   data - original = " https : / / pic4 . zhimg . com / v2 - 4f6462db41dbf60bdd5d232211d0cf7b _ r . jpg " / > ,   第二个 实验 关于 手相 性 （ handedness ） ， 手相 性 不 一致 的 结构 不能 通过 平面 旋转 重合 。 这个 做 有机化学 的 同学 应该 最熟 了 ( 各种 手性 碳 ) ， 比如 被 手 向性 控制 的 恐惧 （ 来报 一下 岩沙 海葵 毒素 的 IUPAC 命名 ？ ） ： ,   < img   src = " https : / / pic2 . zhimg . com / v2 - 78de80a7fea34f1e7628a2331c662749 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 792 "   data - rawheight = " 498 "   class = " origin _ image   zh - lightbox - thumb "   width = " 792 "   data - original = " https : / / pic2 . zhimg . com / v2 - 78de80a7fea34f1e7628a2331c662749 _ r . jpg " / > ,   最 简单 的 手相 性 就是 分清 左右 ， 这个 到 现在 很多 人 都 会 搞混 。 判断 手相 性 对 人 来说 是 很 困难 的 。 Hinton   给 的 例子 是 “ 意识 旋转 ” ( mental   rotation ) ， 这个 问题 是 判断 某 两个 图形 的 手相 性 是否 一致 : ,   < img   src = " https : / / pic4 . zhimg . com / v2 - 67920440e8d68bcebb480f55f928b8e7 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1714 "   data - rawheight = " 1312 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1714 "   data - original = " https : / / pic4 . zhimg . com / v2 - 67920440e8d68bcebb480f55f928b8e7 _ r . jpg " / > ,   ( 图片 取自 Hinton 在 University   of   Toronto   的 名为   Does   the   Brain   do   Inverse   Graphics ?   的 讲座 的 公开 PPT ) ,   我们 无法 直接 回答 ， 而是 要 在 意识 中 “ 旋转 ” 某个 R ， 才能 判断 手相 性 是否 一致 。 并且 角度 差 的 越 大 ， 人 判断 时间 就 越长 。 ,   而 “ 意识 旋转 ” 同样 突出 了 “ 坐标 框架 ” 的 存在 ， 我们 难以 判断 手相 性 ， 是因为 它们 有 不 一致 的 坐标 框架 ， 我们 需要 通过 旋转 把 坐标 框架 变得 一致 ， 才能 从 直觉 上 知道 它们 是否 一致 。 ,   第三个 实验 是 关于 地图 的 。 我们 需要 让 一个 对 地理 不是 特别 精通 ， 但是 有 基础知识 的 人 ， 回答 一个 简单 的 问题 ： ,   下面 的 图案 是 什么 洲 ？ ,   < img   src = " https : / / pic4 . zhimg . com / v2 - c0388717409c5ed21486024662cc0ce7 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 930 "   data - rawheight = " 524 "   class = " origin _ image   zh - lightbox - thumb "   width = " 930 "   data - original = " https : / / pic4 . zhimg . com / v2 - c0388717409c5ed21486024662cc0ce7 _ r . jpg " / > ,   相当 多 的 人 （ 特别 是 凭直觉 直接 答 的 ） 回答 ， 像 澳洲 。 ,   这 是因为 对于 不规则 图案 ， 我们 想当然 地 建立 了 这样 的 坐标 框架 ： ,   < img   src = " https : / / pic3 . zhimg . com / v2 - 1b8e571f1ae5415738be0701bc67ba36 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1152 "   data - rawheight = " 708 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1152 "   data - original = " https : / / pic3 . zhimg . com / v2 - 1b8e571f1ae5415738be0701bc67ba36 _ r . jpg " / > ,   但是 如果 你 这样 建立 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - dfda6bcb45f5ae892ed9e2ba494e095c _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 980 "   data - rawheight = " 608 "   class = " origin _ image   zh - lightbox - thumb "   width = " 980 "   data - original = " https : / / pic1 . zhimg . com / v2 - dfda6bcb45f5ae892ed9e2ba494e095c _ r . jpg " / > ,   就 会 立即 发现 这是 非洲 ， 而且 和澳洲 相差 的 挺 大 。 ,   通过 这 几个 实验 ， Hinton 得出 了 这样 的 结论 ： ,   人 的 视觉 系统 会 建立 “ 坐标 框架 ” ， 并且 坐标 框架 的 不同 会 极大 地 改变 人 的 认知 。 ,   也 就是 人 识别 物体 的 时候 ， 坐标 框架 是 参与 到 识别 过程 中 的 ， 识别 过程 受到 了 空间概念 的 支配 ， 并 不是 独立 的 过程 。 “ 坐标 框架 ” 在 此处 就是 人 的 一种 “ 先验 知识 ” 。 但是 在 CNN 上 却 很 难 看到 类似 “ 坐标 框架 ” 的 东西 。 ,   Hinton   提出 了 一个 猜想 ： ,   物体 和 观察者 之间 的 关系 （ 比如 物体 的 姿态 ） ， 应该 由 一整套 激活 的 神经元 表示 ， 而 不是 由 单个 神经元 ， 或者 一组 粗 编码 （ coarse - coded ， 这里 意思 是 指 类似 一层 中 ， 并 没有 经过 精细 组织 ） 的 神经元 表示 。 只有 这样 的 表示 ， 才能 有效 表达 关于 “ 坐标 框架 ” 的 先验 知识 。 ,   而 这 一整套 神经元 ， Hinton 认为 就是 Capsule 。 ,   同 变性 （ Equivariance ） 和 不变性 （ Invariance ） ,   Hinton   反对   CNN 的 另外 一个 理由 是 ， CNN 的 目标 不 正确 。 问题 主要 集中 在   Pooling   方面 （ 我 认为 可以 推广 到 下 采样 ， 因为 现在 很多 CNN 用 卷积 下 采样 代替 Pooling 层 ） 。 Hinton 认为 ， 过去 人们 对 Pooling 的 看法 是 能够 带来   invariance   的 效果 ， 也 就是 当 内容 发生 很小 的 变化 的 时候 （ 以及 一些 平移 旋转 ） ， CNN   仍然 能够 稳定 识别 对应 内容 。 ,   但是 这个 目标 并 不 正确 ， 因为 最终 我们 理想 的 目标 不是 为了 “ 识别率 ” ， 而是 为了 得到 对 内容 的 良好 的 表示 ( representation ) 。 如果 我们 找到 了 对 内容 的 良好 表示 ， 那么 就 等于 我们 “ 理解 ” 了 内容 ， 因为 这些 内容 可以 被 用来 识别 ， 用来 进行 语义 分析 ， 用来 构建 抽象 逻辑 ， 等等等等 。 而 现在 的   CNN   却 一味 地 追求 识别率 ， 这 不是 Hinton 想要 的 东西 ， Hinton 想要   “ something   big ” 。 ,   Hinton 的 看法 是 ， 我们 需要   Equivariance   而 不是   Invariance 。 ,   所谓   Invariance ， 是 指 表示 不随 变换 变化 ， 比如 分类 结果 等等 。 ,   < img   src = " https : / / pic2 . zhimg . com / v2 - a536961236988dd93c3629c918ced961 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1564 "   data - rawheight = " 866 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1564 "   data - original = " https : / / pic2 . zhimg . com / v2 - a536961236988dd93c3629c918ced961 _ r . jpg " / > ,   Invariance   主要 是 通过   Pooling   等 下 采样 过程 得到 的 。 如果 你 对 训练 神经网络 有 经验 ， 你 可能 会 想到 我们 在 做 图像 预处理 和 数据 拓增 的 时候 ， 会 把 某些 图片 旋转 一些 角度 ， 作为 新 的 样本 ， 给 神经网络 识别 。 这样 CNN 能够 做到 对 旋转 的   invariance ， 并且 是 “ 直觉 上 ” 的 invariance ， 根本 不 需要 像 人 那样 去 旋转 图片 ， 它 直接 就 “ 忽视 ” 了 旋转 ， 因为 我们 希望 它 对 旋转 invariance 。 ,   CNN 同样 强调 对 空间 的   invariance ， 也 就是 对 物体 的 平移 之类 的 不 敏感 （ 物体 不同 的 位置 不 影响 它 的 识别 ） 。 这 当然 极大地提高 了 识别 正确率 ， 但是 对于 移动 的 数据 （ 比如 视频 ） ， 或者 我们 需要 检测 物体 具体 的 位置 的 时候 ， CNN 本身 很难 做 ， 需要 一些 滑动 窗口 ， 或者 R - CNN 之类 的 方法 ， 这些 方法 很 反常 （ 几乎 肯定 在 生物学 中 不 存在 对应 结构 ） ， 而且 极难 解释 为什么 大脑 在 识别 静态 图像 和 观察 运动 场景 等 差异 很大 的 视觉 功能 时 ， 几乎 使用 同 一套 视觉 系统 。 ,   对 平移 和 旋转 的   invariance ， 其实 是 丢弃 了 “ 坐标 框架 ” ， Hinton 认为 这是 CNN 不能 反映 “ 坐标 框架 ” 的 重要 原因 。 ,   而   equivariance   不会 丢失 这些 信息 ， 它 只是 对 内容 的 一种 变换 ： ,   < img   src = " https : / / pic4 . zhimg . com / v2 - eeed752a7450104f43f02725265cf32f _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 2538 "   data - rawheight = " 1414 "   class = " origin _ image   zh - lightbox - thumb "   width = " 2538 "   data - original = " https : / / pic4 . zhimg . com / v2 - eeed752a7450104f43f02725265cf32f _ r . jpg " / > ,   Hinton   认为   CNN   前面 非   Pooling   的 部分 做 的 很 好 ， 因为 它们 是   equivariance   的 。 ,   那么 在   Capsule   的 框架 下 ， 又 应该 如何 体现   equivariance   呢 ？ ,   Hinton   认为 存在 两种   equivariance ： ,   位置 编码 （ place - coded ） ： 视觉 中 的 内容 的 位置 发生 了 较大 变化 ， 则 会 由 不同 的   Capsule   表示 其 内容 。 速率 编码 （ rate - coded ） ： 视觉 中 的 内容 为 位置 发生 了 较 小 的 变化 ， 则 会 由 相同 的   Capsule   表示 其 内容 ， 但是 内容 有所 改变 。 ,   并且 ， 两者 的 联系 是 ， 高层 的   capsule   有 更 广的域   ( domain ) ， 所以 低层 的   place - coded   信息 到 高层 会 变成   rate - coded 。 ,   这里 Hinton 虽然 没有 指明 ， 但是 我 感觉 到   Hinton   是 希望 能够 统一 静态 视觉 和 动态 视觉 的 （ 通过 两种 编码方式 ， 同时 感知 运动 和 内容 ） 。 人脑 中 对于 静态 和 动态 内容 的 处理 通路 并 没有 特别 大 的 变化 ， 但是 现在 做 视频 理解 的 框架 和 做 图片 理解 的 差异 还是 不小 的 。 ,   但是 ， 毕竟   invariance   是 存在 的 ， 比如 我们 对 物体 的 识别 确实 不 和 物体 的 位置 有关 。 这里 Hinton 解释 了 一下 ： ,   knowledge ， but   not   activities   have   to   be   invariant   of   viewpoint ,   也 就是 Hinton 谈论 的 问题 是 关于   activation   的 ， 之前 人们 所说 的 CNN 的   invariance   是 关于 神经元   activation   的 。 Hinton   希望   invariance   仅仅 是 对于   knowledge   的 （ 对于 Capsule 而言 ， 是 其 输出 的 概率 部分 ； 而 其 位置 等 参数 是 equivariant 的 ） 。 通过 这 可以 看到 Hinton 使用 Capsule 的 一个 原因 是 觉得 Capsule 相比 单个 神经元 更 适合 用来 做 表示 。 ,   Capsule   与   coincidence   filtering   （ 巧合 筛分 ） ,   那么 高层 的   Capsule   怎么 从 底层 的   Capsule   获取信息 呢 ？ ,   首先   Capsule   的 输出 是 什么 ？ ,   Hinton   假设   Capsule   输出 的 是   instantiation   parameters   （ 实例 参数 ）   ， 这是 一个 高维 向量 ： ,   其模长 代表 某个 实体 （ 某个 物体 ， 或者 其 一部分 ） 出现 的 概率 其 方向 / 位置 代表 实体 的 一般 姿态   ( generalized   pose ) ， 包括 位置 ， 方向 ， 尺寸 ， 速度 ， 颜色 等等 ,   Capsule   的 核心 观念 是 ， 用 一组 神经元 而 不是 一个 来 代表 一个 实体 ， 且 仅 代表 一个 实体 。 ,   然后 通过 对 底层 的   Capsule   做   coincidence   filtering   （ 巧合 筛分 ） 决定 激活 哪些 高层 的 Capsule   。 coincidence   filtering 是 一种 通过 对 高 维度 向量 进行 聚类 来 判断 置信 的 方式 ， Hinton 举 了 一个 例子 ： ,   比如 你 在 街上 听到 有人 谈论 11 号 的 纽约时报 ， 你 一 开始 可能 并 不在意 ； 但是 如果 你 沿路 听到 4 个 或者 5 个 不同 的 人 在 谈论 11 号 的 纽约时报 ， 你 可能 就 立即 意识 到 一定 有 什么 不 平常 的 事情 发生 了 我们 的 （ 非 日常 ） 语句 就 像 高 维空间 中 向量 ， 一组 相近 语句 的 出现 ， 自然 条件 下 概率 很小 ， 我们 会 很 本能 地 筛分 出 这种 巧合 。 ,   coincidence   filtering   能够 规避 一些 噪声 ， 使得 结果 比较   robust 。 ,   这 让 我 想起 了 现在 CNN 容易 被 对抗 样本 欺骗 的 问题 。 虽然 几乎 所有 的 机器 学习 模型 都 存在 对抗 样本 的 问题 ， 但是 CNN 可以 被 一些 对人 而言 没有 区别 的 对抗 样本 欺骗 ， 这是 严峻 的 问题 （ 这 也 是 CNN 异于 我们 视觉 系统 的 一点 ） 。 一部分 原因 在于 NN 的 线性 结构 ， 其 对 噪声 的 耐受 不是 很 好 。 不 知道   coincidence   filtering   能否 缓解 这个 问题 。 ,   用 图来 表示 就 像 这样 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - e66220dbff591d55582581589a2807b8 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1180 "   data - rawheight = " 928 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1180 "   data - original = " https : / / pic1 . zhimg . com / v2 - e66220dbff591d55582581589a2807b8 _ r . jpg " / > ,   Hinton 采用 的 聚类 （ 他 称为 Agree ） 方式 是 使用 以下 评估 ： ,   ,   其中   mixture   是 gaussian   mixture ， 可以 通过 EM 算法 得到 。 也 就是 ， 如果 簇 的 形状 越 接近 高斯分布 （ 也 就是 越 集中 ） ， 得分 越高 ； 反之 越 分散 越 接近 均匀分布 ， 得分 越低 ： ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 0aedd01607c1ca743ba29fd403b3efe4 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 600 "   data - rawheight = " 451 "   class = " origin _ image   zh - lightbox - thumb "   width = " 600 "   data - original = " https : / / pic1 . zhimg . com / v2 - 0aedd01607c1ca743ba29fd403b3efe4 _ r . jpg " / > ,   ( 图片 取自   Youtube   [ 2 ] ) ,   得到 高分 的 簇 的 分类 所 对应 的 上层 capsule 会 接受 下层 capsule 提供 的 generalized   pose ， 相当于 做 了 routing 。 这 是因为 下层 的 这些 输出 ， “ 选择 ” 了 上层 的 capsule ， “ 找到 最好 的 （ 处理 ） 路径 等价 于 （ 正确 ） 处理 了 图像 ” ， Hinton   这样 解释 。 Hinton   称 这种 机制 为   “ routing   by   agreement ” 。 ,   这种   routing   不是 静态 的 ， 而是 动态 的 （ 随 输入 决定 的 ） ， 这是   Pooling   等 方式 不 具备 的 ： ,   < img   src = " https : / / pic4 . zhimg . com / v2 - d7d59a6f7b8becff53c58ff9be232d0f _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 600 "   data - rawheight = " 428 "   class = " origin _ image   zh - lightbox - thumb "   width = " 600 "   data - original = " https : / / pic4 . zhimg . com / v2 - d7d59a6f7b8becff53c58ff9be232d0f _ r . jpg " / > ,   由于 使用 这种 类似 聚类 的 方式 ， 其有 潜在 的   unsupervised   learning   的 能力 ， 不过 Hinton 还 没有 透露 具体 的 算法 。 但是   Hinton   在   [ 2 ]   中 提到 ， 对于   MNIST   数据 集 ， 经过   unsupervised   learning   后 ， 只 需要 25 个 例子 ， 就 可以 达到 98.3% 的 识别 准确率 ， 并且 解决 了 CNN 识别 重叠 图像 困难 等 问题 。 这些 应该 在 最近 被   NIPS   接受 的 关于   Capsules   论文   Dynamic   Routing   between   Capsules   （ 尚未 发表 ） https : / / research . google . com / pubs / pub46351 . html   中 可以 看到 。 让 我们 拭目以待 。 ,   图形学 和 线性 流形 （ linear   manifold ） ,   Hinton   这次 明显 受到 了 计算机 图形学 的 启发 。 他 在 报告 [ 2 ] 中说   literally ， literally ,   reverse   of   graphics .   （ 我 非常 非常 认真 地 想要 “ 逆向 ” 图形学 ） 。 ,   计算机 图形学 中有 个 非常 重要 的 性质 ， 就是 其 使用 了   linear   manifold ， 有 良好 的 视角 不变性 。 ,   说 明白 一点 ， 也 就是 用 视角 变换 矩阵 作用 到 场景 中 ， 不 改变 场景 中 物体 的 相对 关系 。 ,   于是 Hinton 决定 用 矩阵 处理 两个 物体 间 的 关联 。 ,   按照 上面 的   routing   by   agreement   的 算法 ， 如果 我们 希望 从   mouth   和   nose   得到   face ， 我们 需要 让 mouth 的 向量     和   nose   的 向量     基本一致 。 ,   它们 本身 肯定 不会 一致 的 ， 因为   mouth   和   nose   不是 一样 的 东西 ； 要 让 它们 一致 我们 就 需要 找 一类 函数 ， 使得     。 ,   但是 选择 哪类 函数 呢 ？ Hinton 的 答案 是 多重 线性 函数 （ 矩阵 ） ， 因为 这 能够 使得 它们 的 关系 不 受 视角 变换 （ 设 视角 变换 为 矩阵 W ） 影响 ， 这 是因为 ,   ,   < img   src = " https : / / pic2 . zhimg . com / v2 - 1963f0505e3bd97e2a38f18cc67376e1 _ b . png "   data - caption = " "   data - size = " normal "   data - rawwidth = " 1758 "   data - rawheight = " 1316 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1758 "   data - original = " https : / / pic2 . zhimg . com / v2 - 1963f0505e3bd97e2a38f18cc67376e1 _ r . jpg " / > ,   ( 图片 取自 Hinton 在 University   of   Toronto   的 名为   Does   the   Brain   do   Inverse   Graphics ?   的 讲座 的 公开 PPT ) ,   而且 这 对 三维 也 是 有效 的 ， 这里 看到 了   Hinton   冲击 三维 视觉 的 野心 。 ,   Hinton   这波会 成功 吗 ？ ,   Hinton   是 个 很 “ 固执 ” 的 人 ， 在   Andrew   Ng   对 他 的 采访 中 ， 他 说出 了 自己 的 想法 ： ,   " If   your   intuitions   are   good ,   you   should   follow   them   and   you   will   eventually   be   successful ;   if   your   intuitions   are   not   good ,   it   doesnt   matter   what   you   do .   You   might   as   well   trust   your   intuitions   theres   no   point   not   trusting   them . " ,   （ 意思 是 如果 直觉 一直 很 好 ， 那么 当然 应该 坚持 ； 如果 直觉 很差 ， 那么 怎么 做 也 没有 关系 了 （ 反正 你 也 搞 不出 什么 ， 即使 你 换个 想法 大抵 也 不会 成功 ） ） 。 当然 后 半句 可能 是   Hinton   的 高级 黑 。 ,   但是   Hinton   确乎 坚信 自己 的 直觉 ， 从 反向 传播 提出 ， 到 深度 学习 的 火爆 ， Hinton 已经 坚守 了 30 年 了 ， 并 没有 任何 放弃 的 意思 。 ,   现在   Capsule   给 了   Hinton   很多 直觉 ， Hinton   估计 也 是 会 一条 路 走 到 黑 。 Hinton   的 目标 也 很大 ， 从 他 对   capsule   的 介绍 中 可以 看到 有 冲击 动态 视觉 内容 、 3D 视觉 、 无 监督 学习 、 NN 鲁棒性 这 几个 “ 老大难 ” 问题 的 意思 。 ,   如果 Hinton 会 失败 （ 我 不是 不 看好 Hinton ， 而是 仅仅 做 一个 假设 ） ， 大抵 是 两种 情况 ， ,   第一种 是因为 现在 反向 传播 的 各种 优点 ， 上面 已经 总结 过 了 。 一个 模型 要 成功 ， 不仅 要求 效果 良好 ， 还 要求 灵活性 （ 以便 应用 在 实际 问题 中 ） ， 高效性 ， 和 社区 支持 （ 工业界 和 学术界 的 采纳 程度 和 热门 程度 ） 。 现在 的 反向 传播 在 这 几点 上 都 非常   promising ， 不 容易 给 其他 模型 让步 。 ,   第二种 是因为 即使 一个 直觉 特别 好 的 人 ， 也 有 可能 有 直觉 特别 不好 的 一天 ， 尤其 是 晚年 。 这点 非常 著名 的 例子 是 爱因斯坦 。 爱因斯坦 性格 和   Hinton   很 像 ， 有 非常 敏锐 的 直觉 ， 并且 对 自己 的 直觉 的 值守 到 了 近乎 固执 的 程度 。 爱因斯坦 晚年 的 时候 ， 想要 搞 统一 场论 ， 这是 一个 很大 的 目标 ， 就 好像 现在 Hinton 希望 能够 创造 颠覆 BP机 制 的 目标 一样 ； 爱因斯坦 也 获得 了 很多 直觉 ， 比如 他 觉得 电磁场 和 引力 是 非常 相似 的 ， 都 和 相对论 紧密 关联 ， 都 是 平方 反比 ， 都 是 一种 传递 力 的 波 色子 ， 并且 玻色子 静 质量 都 是 0 ， 力 的 范围 都 是 无穷远 ， 等等等等 ， 就 好像 现在 Hinton 找到 的 各种各样 很 有 说服力 的 论据 一样 ； 于是 爱因斯坦 决定 首先 统一 电磁力 和 引力 ， 结果 是 失败 的 。 反而 是 两种 看上去 很 不 搭 的 力 — — 弱 相互作用 力 （ 3 种 玻色子 ， 范围 在 原子核 大小 内 ） 和 电磁力 首先 被 统一 了 （ 电弱 统一 理论 ） 。 而 引力 恰恰 是 目前 最 难 统一 的 ， 也 就是 爱因斯坦 的 直觉 走 反 了 。 我 很 担心   Hinton   也 会 如此 。 ,   不过 即使 爱因斯坦 没有 成功 ， 后人 也 为 其 所 激励 ， 继续 扛起 GUT 的 大旗 推动 物理 前沿 ； 对于 Hinton   我 想 也 是 一样 。 ,   尾注 ,   Hinton   曾经 在 1987 年 左右 发明 了   recirculation   算法 代替 BP 来 训练 神经网络 ， 虽然 不算 特别 成功 ， 但是 却 预言 了 后来 神经科学 才 发现 的 spike   timing - independent   plasticity 。 ,   Hinton   最初 提出   capsule   的 时候 ( 5 年前 ) ， 几乎 “ 逢 投必 拒 ” ， 没有 人 相信 ， 但是   Hinton   自己 一致 坚信 这 一点 ， 并且 一直 坚持 到 现在 。 ,   在   Andrew   Ng 对 其 采访 中 ， Hinton   对 未来 的 趋势 （ 对 CS 从业者 ） 的 一句 很 有意思 的 描述 ： showing   computers   is   going   to   be   as   important   as   programming   computers . ,   Reference ,   [ 1 ]   2017 年 8 月 17 日 ， 于 加拿大多伦多大学   Fields   Institute ， Hinton   的 报告     https : / / www . youtube . com / watch ? v = Mqt8fs6ZbHk ,   [ 2 ]   2017 年 4 月 3 日 发布   Brain   &   Cognitive   Sciences   于   MIT ， Hinton   的 报告   https : / / www . youtube . com / watch ? v = rTawFwUvnLE ,   [ 3 ]   媒体报道   Hinton   要 将 当前 的 深度 学习 核心 算法 推倒重来   Artificial   intelligence   pioneer   says   we   need   to   start   over ,   " [ 4 ]   Fei - Fei   Li   在   Twitter   上 的 评论 ： Echo   Geoffs   sentiment   no   tool   is   eternal ,   even   backprop   or   deeplearning .   V .   important   to   continue   basic   research . " ,   [ 5 ]   Le ,   Q .   V .   ( 2013 ,   May ) .   Building   high - level   features   using   large   scale   unsupervised   learning .   In   Acoustics ,   Speech   and   Signal   Processing   ( ICASSP ) ,   2013   IEEE   International   Conference   on   ( pp .   8595 - 8598 ) .   IEEE . ,   [ 6 ]   B é ny ,   C .   ( 2013 ) .   Deep   learning   and   the   renormalization   group .   arXiv   preprint   arXiv : 1301.3124 . ,   [ 7 ]   Hinton ,   G .   ( 2010 ) .   A   practical   guide   to   training   restricted   Boltzmann   machines .   Momentum ,   9 ( 1 ) ,   926 . ,   [ 8 ]   Rina   Decher   ( 1986 ) .   Learning   while   searching   in   constraint - satisfaction   problems .   University   of   California ,   Computer   Science   Department ,   Cognitive   Systems   Laboratory . ,   [ 9 ]   Hinton ,   G .   E . ,   Krizhevsky ,   A . ,   &   Wang ,   S .   D .   ( 2011 ,   June ) .   Transforming   auto - encoders .   In   International   Conference   on   Artificial   Neural   Networks   ( pp .   44 - 51 ) .   Springer   Berlin   Heidelberg . ,   [ 10 ]   Wolpert ,   D .   H .   ( 1996 ) .   The   lack   of   a   priori   distinctions   between   learning   algorithms .   Neural   computation ,   8 ( 7 ) ,   1341 - 1390 . ,   [ 11 ]   Hinton ,   G .   E . ,   &   McClelland ,   J .   L .   ( 1988 ) .   Learning   representations   by   recirculation .   In   Neural   information   processing   systems   ( pp .   358 - 366 ) . ,   [ 12 ]   Pathak ,   D . ,   Agrawal ,   P . ,   Efros ,   A .   A . ,   &   Darrell ,   T .   ( 2017 ) .   Curiosity - driven   exploration   by   self - supervised   prediction .   arXiv   preprint   arXiv : 1705.05363 . ]