[ < img   src = " https : / / pic1 . zhimg . com / v2 - 1694e0f7958cebf6687253be1cbe5618 _ b . jpg "   data - rawwidth = " 385 "   data - rawheight = " 300 "   class = " content _ image "   width = " 385 " / > ,   ,   公元 七 世纪 ， 在 车 迟国 国家 气象局 组织 的 一次 求雨 活动 中 ， 虎力 、 鹿力 、 羊力 三位 大仙 成功 地 祈下 甘霖 ， 于 水火 中救 了 黎民 。 老 国王 虽然 不明 就 里 ， 却 从此 尊 他们 为 国师 ， 奉 道教 为 圭臬 。 ,   本世纪 ， 算法 工程师 们 的 境遇 也 差不多 ： 早些 年 ， 信奉 糙快 猛 主义 的 大佬 们 觉得 他们 饱食终日 、 无所用心 ， 没 工作 只好 在 学校 混 博士 ， 靠 数据 上 的 障眼法 装神弄鬼 。 可是 ， 随着 去年 AlphaGo 大破 李世石 ， 大佬 们 在 心底 喊 出 “ 我操 ” 的 同时 ， 慌不择路 地 把 各种 搞 劫持 、 送 外卖 的 生意 包装 成 人工智能 ， 并 纷纷 请来 几位 懂 算法 的 国师 加持 。 虽然 他们 对 国师 们 所 做 的 事 智商 上 并 不 理解 ， 却 虔诚地 希望 他们 快点儿 求 下雨 来 。 ,   于是 ， 算法 工程师 的 身价 也 水涨船高 了 。 各门派 工程师 不论 过去 练 的 是 java 、 php 还是 excel ， 都 放弃 了 最好 语言 的 争论 ， 抄起 了 深度 学习 ， 发誓 重新 修炼 成 算法 工程师 。 前些天 ， 还有 人 在 知乎 上 问 我 ： 20 万 、 50 万 、 100 万 的 算法 工程师 ， 到底 有 什么 区别 ？ ,   这样 充满 铜臭味 儿 的 问题 ， 让 我 十分 欣慰 。 虽说 在 北京 ， 20 万 已经 基本 不 可能 招到 靠 谱儿 的 算法 工程师 了 ， 还是 姑且 用 上面 的 数字 做个 参照 ， 谈谈 算法 工程师 的 三个 层次 吧 。 （ 这里 说 的 算法 ， 并 不是 计算机系 本科课程 《 算法 与 数据结构 》 里 那个 算法 。 那门 课里 讲 的 ， 是 排序 、 查找 这 类 " 确定性 算法 " ； 而 这里 我们 说 的 ， 是 用 统计 方法 对 数据 进行 建模 的 " 概率 性 算法 " 。 ） 下文 中 会 提到 一些 算法 和 模型 ， 但 不过 是 为了 举例说明 概念 ， 无需 深究 ， 有 兴趣 钻研 的 朋友 可以 自己 查阅 资料 。 ,   第一 层次 " Operating " ： 会 使用 工具 ,   这个 层次 的 工程师 ， 对 常用 的 模型 比较 熟悉 ， 来 了 数据 以后 ， 好歹 能 挑个 合适 的 跑 一下 。 ,   达到 这个 层次 ， 其实 门槛 不高 。 早些 年 ， 您 只要 掌握 了 什么 叫 LDA 、 哪 叫 SVM ， 再玩过 几次 libnear 、 mahout 等 开源 工具 ， 就 可以 拿到 数据 后 跑 个 结果 出来 。 到 了 深度 学习 时代 ， 这件 事儿 似乎 就 更 简单 了 ： 管它 什么 问题 ， 不 都 是 拿 神经网络 往上 堆 嘛 ！ 最近 ， 经常 会 遇到 一些 工程师 ， 成功 地 跑 通 了 Tensorflow 的 demo 后 ， 兴高采烈 地 欢呼 ： 我 学会 深度 学习 了 ， 我 明天 就 统治 人类 了 ！ ,   这事要 真 这么 简单 ， 我 是 茄子 。 任凭 你 十八般 开源 工具 用 的 再 熟 ， 也 不 可能 搞 出个 战胜 柯洁 的 机器人 来 。 这里 要 给 大家 狠狠 浇上 一盆 冷水 ： 进入 这个 领域 的 人 ， 都 要 先 了解 一个 “ 没有 免费 的 午餐 定理 ” ， 这个 定理 的 数学 表达 过于 晦涩 ， 我们 把 它 翻译成 并 不 太 准确 的 文艺语言 ： ,   如果 有 两个 模型 搞 一次 多 回合 的 比武 ， 每个 回合 用 的 数据 集 不同 ， 而且 数据 集 没什么 偏向 性 ， 那么 最后 的 结果 ， 十有八九 是 双方 打平 。 ,   管 你 是 普通 模型 、 文艺 模型 还是 2B 模型 ， 谁 也 别 瞧不起 谁 。 考虑 一种 极端 情况 ： 有 一个 参赛 模型 是 “ 随机 猜测 ” ， 也 就是 无 根据地 胡乱 给 个 答案 ， 结果 如何 呢 ？ 对 ， 还是 打平 ！ 所以 ， 请 再也 不要 问 “ 聚类 用 什么 算法 效果 好 ” 这样 的 傻 问题 了 。 ,   这 就 很 尴尬 了 ！ 因为 掌握 了 一堆 模型 并且 会 跑 ， 其实 并 没有 什么 卵用 。 当然 ， 实际 问题 的 数据分布 ， 总是 有 一定 特点 的 ， 比方说 人脸识别 ， 图 中间 怎么 说 都 得 有 个 大 圆饼 。 因此 ， 问 “ 人脸识别 用 什么 模型 好 ” 这样 的 问题 ， 就 有 意义 了 。 而 算法 工程师 的 真正 价值 ， 就是 洞察 问题 的 数据 先验 特点 ， 把 他们 表达 在 模型 中 ， 而 这个 ， 就 需要 下 一个 层次 的 能力 了 。 ,   会 使用 工具 ， 在 算法 工程师 中 仅仅 是 入门 水平 ， 靠 这 两把刷子 解决问题 ， 就 好比 杀过 两只 鸡 就 想 做 腹腔 手术 一样 ， 不靠 谱儿 程度 相当 高 。 如果 不是 在 薪酬 膨胀 严重 的 互联网界 ， 我 觉得 20 万是 个 比较 合理 的 价格 。 ,   ,   第二 层次 " Optimization " ： 能 改造 模型 ,   这个 层次 的 工程师 ， 能够 根据 具体 问题 的 数据 特点 对模型 进行 改造 ， 并 采用 相应 合适 的 最优化 算法 ， 以 追求 最好 的 效果 。 ,   不论 前人 的 模型 怎么 美妙 ， 都 是 基于 当时 观察 到 的 数据 先验 特点 设计 的 。 比如说 LDA ， 就是 在 语料 质量 不高 的 情况 下 ， 在 PLSA 基础 上 引入 贝叶斯 估计 ， 以 获得 更加 稳健 的 主题 。 虽说 用 LDA 不会 大错 ， 但是 要 在 你 的 具体 问题 上 跑 出 最好 的 效果 ， 根据 数据 特点 做 模型 上 的 精准 改造 ， 是 不可避免 的 。 ,   互联网 数据 这一 现象 更加 明显 ， 因为 没有 哪 两家 公司 拥有 的 数据 是 相似 的 。 百度 的 点击率 模型 ， 有 数十亿 的 特征 ， 大规模 的 定制 计算 集群 ， 独特 的 深度 神经网络 结构 ， 你 能 抄 么 ？ 抄过来 也 没用 。 用 教科书 上 的 模型 不变 应万变 ， 结果 只能 是 刻舟求剑 。 ,   改造 模型 的 能力 ， 就 不是 用 几个 开源 工具 那么 简单 了 ， 这 需要 有 两 方面 的 素养 ： ,   一 、 深入 了解 机器 学习 的 原理 和 组件 。 机器 学习 领域 ， 有 很多 看似 不 那么 直接 有用 的 基础 原理 和 组件 。 比方说 ， 正则 化 怎么 做 ？ 什么 时候 应该 选择 什么样 的 基本 分布 ？ ( 如下 表 )   贝叶斯 先验 该 怎么 设 ？ 两个 概率分布 的 距离 怎么 算 ？ 当 你 看到 前辈 高人 把 这些 材料 烹调 在 一起 ， 变成 LDA 、 CNN 这些 成品 菜肴 端上来 的 时候 ， 也 要 想想 如果 自己 下厨 ， 是否 了解 食材 ， 会 不会 选择 和 搭配 。 仅仅 会 吃 几个 菜 ， 说出 什么 味道 ， 离好 厨师 差 的 还 远 着 呢 。 ,   < img   src = " https : / / pic2 . zhimg . com / v2 - 2e6654cb4bd3f7d8d8b344b8da088f59 _ b . jpg "   data - rawwidth = " 1286 "   data - rawheight = " 381 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1286 "   data - original = " https : / / pic2 . zhimg . com / v2 - 2e6654cb4bd3f7d8d8b344b8da088f59 _ r . jpg " / > ,   二 、 熟练掌握 最优化 方法 。 机器 学习 从业者 不 懂 最优化 ， 相当于 武术家 只会 耍 套路 。 这 就 跟 雷公 太极 和 闫芳 大师 一样 ， 实战 起来 一定 是 鼻青脸肿 。 管 你 设计 了 一个多 牛 逼 的 模型 ， 如果 无法 在 有限 的 计算资源 下 找出 最优 解 ， 那么 不过 是 个 花瓶 罢了 。 ,   最优化 ， 是 机器 学习 最 、 最 、 最 重要 的 基础 。 你 要 知道 ， 在 目标 函数 及其 导数 的 各种 情形 下 ， 应该 如何 选择 优化 方法 ； 各种 方法 的 时间 空间 复杂度 、 收敛性 如何 ； 还要 知道 怎样 构造 目标 函数 ， 才 便于 用 凸 优化 或 其他 框架 来 求解 。 而 这些 方面 的 训练 ， 要 比 机器 学习 的 模型 还要 扎实 才行 。 ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 37dfc082fe36ff76e6afe5b5f67b64b8 _ b . jpg "   data - rawwidth = " 1240 "   data - rawheight = " 464 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1240 "   data - original = " https : / / pic1 . zhimg . com / v2 - 37dfc082fe36ff76e6afe5b5f67b64b8 _ r . jpg " / > ,   拿 大家 以为 " 以不变应万变 " 的 深度 学习 举个 例子 。 用 神经网络 处理 语音 识别 、 自然语言 处理 这种 时间 序列 数据 的 建模 ， RNN （ 见上图 ） 是 个 自然 的 选择 。 不过 在实践中 ， 大家 发现 由于 “ 梯度 消失 ” 现象 的 存在 ， RNN 很难 对 长程 的 上下文 依赖 建模 。 而 在 自然语言 中 ， 例如 决定 下面 的 be 动词 是 “ is ” 还是 “ are ” 这样 的 问题 ， 有 可能 往前 翻 好多 词 才能 找到 起 决定 作用 的 主语 。 怎么办 呢 ？ 天才 的 J .   Schmidhuber 设计 了 带有 门 结构 的 LSTM 模型 （ 见 下图 ） ， 让 数据 自行决定 哪些 信息 要 保留 ， 那些 要 忘掉 。 如此 以来 ， 自然语言 的 建模 效果 ， 就 大大提高 了 。 大家 初看 下面 两张 RNN 与 LSTM 的 结构 对比 ， 面对 凭空 多 出来 的 几个 门 结构 可能 一头雾水 ， 唯有 洞彻 其中 的 方法论 ， 并且 有 扎实 的 机器 学习 和 最优化 基础 ， 才能 逐渐 理解 和 学习 这种 思路 。 ,   < img   src = " https : / / pic4 . zhimg . com / v2 - c78d63c52739e465c6f3ac856358e80b _ b . jpg "   data - rawwidth = " 1240 "   data - rawheight = " 466 "   class = " origin _ image   zh - lightbox - thumb "   width = " 1240 "   data - original = " https : / / pic4 . zhimg . com / v2 - c78d63c52739e465c6f3ac856358e80b _ r . jpg " / > ,   当然 ， LSTM 这个 模型 是 神来之笔 ， 我 等 对此 可望 不可 及 。 不过 ， 在 这个 例子 里 展现 出来 的 关键 能力 ： 根据 问题 特点 调整 模型 ， 并 解决 优化 上 的 障碍 ， 是 一名 合格 的 算法 工程师 应该 追求 的 能力 。 年薪 50 万能 找到 这样 的 人 ， 是 物有所值 的 。 ,   第三 层次 " Objective " ： 擅 定义 问题 ,   这个 层次 的 工程师 （ 哦 ， 似乎 叫 工程师 不太 合适 了 ） ， 扔给 他 一个 新 的 实际 问题 ， 可以 给出 量化 的 目标 函数 。 ,   当年 ， 福特公司 请人 检修 电机 ， 斯坦门 茨 在 电机 外壳 画 了 一条线 ， 让 工作人员 在 此处 打开 电机 迅速 排除 了 故障 。 结账 时 ， 斯坦门茨要 1 万美元 ， 还 开了个 清单 ： 画 一条线 ， 1 美元 ； 知道 在 哪儿 画线 ， 9999 美元 。 ,   同样 的 道理 ， 在 算法 领域 ， 最难 的 也 是 知道 在 哪里 画线 ， 这 就是 对 一个 新 问题 构建 目标 函数 的 过程 。 而 有 明确 的 量化 目标 函数 ， 正是 科学 方法 区别 于 玄学 方法 、 神学 方法 的 重要 标志 。 ,   目标 函数 ， 有时 能 用 一个 解析 形式 （ Analytical   form ） 写 出来 ， 有时 则 不能 。 比方说 网页 搜索 这个 问题 ， 有 两种 目标 函数 ： 一种 是 nDCG ， 这是 一个 在 标注 好 的 数据 集上 可以 明确 计算出来 的 指标 ； 另 一种 则 是 人工 看 badcase 的 比例 ， 显然 这个 没法用 公式 计算 ， 但是 其 结果 也 是 定量 的 ， 也 可以 作为 目标 函数 。 ,   定义 目标 函数 ， 初 听 起来 并 没有 那么 困难 ， 不 就是 制定 个 KPI 么 ？ 其实不然 ， 要 做好 这件 事 ， 在 意识 和 技术 上 都 有 很 高 的 门槛 。 ,   一 、 要 建立 “ 万般皆下品 、 唯有 目标 高 ” 的 意识 。 一个 团队 也好 、 一个 项目 也好 ， 只要 确立 了 正确 的 、 可 衡量 的 目标 ， 那么 达到 这个 目标 就 只是 时间 和 成本 的 问题 。 假设 nDCG 是 搜索 的 正确 目标 函数 ， 那么 微软 也好 、 Yahoo ! 也好 ， 迟早 也 能 追上 Google ， 遗憾 的 是 ， nDCG 这个 目标 是 有点儿 问题 的 ， 所以 后来 这 两家 被 越拉越 远 。 ,   所谓 “ 本立 而 道生 ” ： 一个 项目 开始 时 ， 总是 应该 先 做 两件事 ： 一是 讨论 定义 清楚 量化 的 目标 函数 ； 二是 搭建 一个 能够 对 目标 函数 做线 上 A / B 测试 的 实验 框架 。 而 收集 什么 数据 、 采用 什么 模型 ， 倒 都 在 其次 了 。 ,   二 、 能够 构造 准确 ( 信 ) 、 可解 ( 达 ) 、 优雅 ( 雅 ) 的 目标 函数 。 目标 函数 要 尽可能 反应 实际 业务 目标 ， 同时 又 有 可行 的 优化 方法 。 一般来说 ， 优化 目标 与 评测 目标 是 有所不同 的 。 比如说 在 语音 识别 中 ， 评测 目标 是 “ 词 错误率 ” ， 但 这个 不可 导 所以 没法 直接 优化 ； 因此 ， 我们 还要 找 一个 “ 代理 目标 ” ， 比如 似然值 或者 后验 概率 ， 用于 求解 模型 参数 。 评测 目标 的 定义 往往 比较 直觉 ， 但是 要 把 它 转化成 一个 高度 相关 ， 又 便于 求解 的 优化 目标 ， 是 需要 相当 的 经验 与 功力 的 。 在 语音 建模 里 ， 即便 是 计算 似然值 ， 也 需要 涉及 Baum - Welch 等 比较复杂 的 算法 ， 要 定义 清楚 不是 简单 的 事儿 。 ,   优雅 ， 是 个 更 高层次 的 要求 ； 可是 在 遇到 重大 问题 时 ， 优雅 却 往往 是 不二法门 。 因为 ， 往往 只有 漂亮 的 框架 才 更 接近 问题 的 本质 。 关于 这点 ， 必须 要 提 一下 近年来 最让人 醍醐灌顶 的 大作 — — 生成 对抗 网络 （ GAN ） 。 ,   GAN 要 解决 的 ， 是 让 机器 根据 数据 学会 画画 、 写文章 等 创作 性 问题 。 机器 画画 的 目标 函数 怎么 定 ？ 听 起来 是 一头雾水 。 我们 早年 做 类似 的 语音 合成 问题 时 ， 也 没什么 好 办法 ， 只能 找人 一句句 听 来 打分 。 令人 拍案叫绝 的 是 ， Ian   GoodFellow 在 定义 这个 问题 时 ， 采取 了 下图 的 巧妙 框架 ： ,   < img   src = " https : / / pic3 . zhimg . com / v2 - a25e966667525808ab4ef6538bc3d56e _ b . png "   data - rawwidth = " 502 "   data - rawheight = " 427 "   class = " origin _ image   zh - lightbox - thumb "   width = " 502 "   data - original = " https : / / pic3 . zhimg . com / v2 - a25e966667525808ab4ef6538bc3d56e _ r . jpg " / > ,   既然 靠 人 打分 费时费力 ， 又 不 客观 ， 那 就 干脆 让 机器 打分 把 ！ 好 在 让 机器 认 一幅 特定 语义 的 图画 ， 比如说 人脸 ， 在 深度 学习 中 已经 基本 解决 了 。 好 ， 假设 我们 已经 有 一个 能 打分 的 机器 D ， 现在 要 训练 一个 能 画画 的 机器 G ， 那 就让 G 不断 地画 ， D 不断 地 打分 ， 什么 时候 G 的 作品 在 D 那里 得分 高 了 ， 就算 是 学成 了 。 同时 ， D 在 此 过程 中 也 因为 大量 接触 仿品 而 提升 了 鉴赏能力 ， 可以 把 G 训练 得 更好 。 有 了 这样 定性 的 思考 还 不够 ， 这样 一个 巧妙 设计 的 二人 零 和 博弈 过程 ， 还 可以 表示 成 下面 的 数学 问题 ： ,   ,   < img   src = " https : / / pic1 . zhimg . com / v2 - 0ec4acf9c082afd830ef0e2367a68728 _ b . jpg "   data - rawwidth = " 893 "   data - rawheight = " 141 "   class = " origin _ image   zh - lightbox - thumb "   width = " 893 "   data - original = " https : / / pic1 . zhimg . com / v2 - 0ec4acf9c082afd830ef0e2367a68728 _ r . jpg " / > ,   ,   一个 团队 的 定海神针 ， 就是 能 把 问题 转化成 目标 函数 的 那个 人 — — 哪怕 他 连 开源 工具 都 不会 用 。 100 万 找到 这样 的 人 ， 可 真是 捡了个 大 便宜 。 ,   在 机器 学习 领域 ， 算法 工程师 脚下 的 进阶 之路 是 清晰 的 ： 当 你 掌握 了 工具 、 会 改造 模型 ， 进而 可以 驾驭 新 问题 的 建模 ， 就 能 成长 为 最 优秀 的 人才 。 沿着 这条 路 踏踏实实 走 下去 ， 100 万 并 不是 什么 问题 。 什么 ？ 您 说 还有 300 万 的 呢 ？ 这个 不用 眼热 ， 人家 只不过 把 你 写 代码 的 时间 都 用来 跳槽 了 而已 。 ,   ------------------------------------- ,   我 是 @ 北冥乘 海生   ， 想 吸收 更多负 能量 ， 请 大家 关注 我 的 公众 号 “ 计算 广告 ” ( Comp _ Ad ) 和 知乎 专栏 “ 计算 广告 ” ！ ]